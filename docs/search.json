[{"path":"index.html","id":"overview","chapter":"Overview","heading":"Overview","text":"textbook approaches statistical analysis General Linear Model, taking simulation-based approach R software environment. overarching goal teach students translate description design study linear model analyze data study. focus skills needed analyze data psychology experiments.following topics covered:linear modeling workflow;variance-covariance matrices;multiple regression;interactions (continuous--categorical; categorical--categorical);linear mixed-effects regression;generalized linear mixed-effects regression.material course forms basis one-semester course third-year undergradautes taught Dale Barr University Glasgow School Psychology. part PsyTeachR series course materials developed University Glasgow Psychology staff.Unlike textbooks may encountered, interactive textbook. chapter contains embedded exercises well web applications help students better understand content. interactive content work access material web browser. Printing material recommended. want access textbook without internet connection local version keep case site changes moves, can download version offline use. Just extract files ZIP archive, locate file index.html docs directory, open file using web browser.","code":""},{"path":"index.html","id":"how-to-cite-this-book","chapter":"Overview","heading":"How to cite this book","text":"Barr, Dale J. (2021). Learning statistical models simulation R: interactive textbook. Version 1.0.0. Retrieved https://psyteachr.github.io/stat-models-v1.","code":""},{"path":"index.html","id":"found-an-issue","chapter":"Overview","heading":"Found an issue?","text":"find errors typos, questions suggestions, please file issue https://github.com/psyteachr/stat-models-v1/issues. Thanks!","code":""},{"path":"index.html","id":"information-for-educators","chapter":"Overview","heading":"Information for educators","text":"free re-use modify material textbook purposes, stipulation cite original work. Please note additional terms Creative Commons CC--SA 4.0 license governing re-use material.book built using R bookdown package. source files available github.","code":""},{"path":"introduction.html","id":"introduction","chapter":"1 Introduction","heading":"1 Introduction","text":"","code":""},{"path":"introduction.html","id":"goal-of-this-course","chapter":"1 Introduction","heading":"1.1 Goal of this course","text":"goal course teach analyze (simulate!) various types datasets likely encounter psychologist. focus behavioral data usually collected context planned study experiment—response time, perceptual judgments, choices, decisions, Likert ratings, eye movements, hours sleep, etc.course attempts teach analytical techniques flexible, generalizabile, reproducible. techniques learn flexible sense can applied wide variety study designs different types data. maximally generalizable attempting fully account potential biasing influence sampling statistical inference—, help support claims go beyond particular participants stimuli involved experiment. Finally, approach learn strives fully reproducible, inasmuch analyses unambiguously document every single step transformation raw data research results plain-text script using R code.","code":""},{"path":"introduction.html","id":"generalized-linear-mixed-effects-models-glmms","chapter":"1 Introduction","heading":"1.2 Generalized Linear Mixed-Effects Models (GLMMs)","text":"course emphasizes flexible regression modeling framework rather teaching 'recipes' dealing different types data, assumes fundamental type data need analyze multilevel also need deal continuous measurements also ordinal measurements (ratings Likert scale), counts (number times particular event types taken place), nominal data (category something falls ).end course, learned use Generalized Linear Mixed-Effects Models (GLMMs) quantify relationships dependent variable set predictor variables. understand GLMMs, learn three parts:\"linear model\" part, including capture different types \npredictor variables interactions;\"mixed\" part, including ideas use random effects\nrepresent multilevel dependencies arising repeated\nmeasurements subjects stimuli; andthe \"generalized\" part, extend linear models represent\ndependent variables strictly normal, including count,\nordinal, binary variables.","code":""},{"path":"introduction.html","id":"linear-models","chapter":"1 Introduction","heading":"1.2.1 Linear models","text":"GLMMs extension General Linear Model underlies simpler approaches ANOVA, t-test, classical regression. main conceit course almost data almost design might encounter, can analyzed using GLMMs.simple example linear model \\[Y_i = \\beta_0 + \\beta_1 X_i + e_i\\]\\(Y_i\\) measured value dependent variable observation \\(\\), modeled terms intercept term plus effect predictor \\(X_i\\) weighted coefficient \\(\\beta_1\\) error term \\(e_i\\). Simulated data representing linear relationship \\(Y_i = 3 + 2X_i + e_i\\) shown Figure 1.1.\nFigure 1.1: Simulated data illustrating linear model Y = 3 + 2X\nmight recognize equation representing equation line (\\(y = mx + b\\)), \\(\\beta_0\\) playing role y-intercept \\(\\beta_1\\) playing role slope. \\(e_i\\) term model error observation \\(\\), far away observation \\(Y_i\\) model's prediction given \\(X_i\\).Notation ConventionsThe math equations textbook obey following conventions.Greek letters (\\(\\beta\\), \\(\\rho\\), \\(\\tau\\)) represent population parameters, typically unobserved estimated data. want distinguish estimated parameter true value, use \"hat\": instance, \\(\\hat{\\beta}_0\\) value \\(\\beta_0\\) estimated data.Uppercase Latin letters (\\(X\\), \\(Y\\)) represent observed values—.e., things measured, whose values therefore, known. also see lowercase Latin letters (e.g., \\(e_i\\)) represent statistical errors things refer 'derived' 'virtual' quantities (explained later course).\"linear\" linear models mean think means!Many people assume 'linear models' can capture linear relationships, .e., relationships can described straight line (plane). false.linear model weighted sum various terms, term single predictor (constant) multiplied coefficient. model , coefficients \\(\\beta_0\\) \\(\\beta_1\\). can fit kinds complicated relationships linear models, including relationships non-linear, like shown .\nFigure 1.2: Nonlinear relationships can modeled using linear model.\nleft panel, capture quadratic (parabolic) function using linear model \\(Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2\\). relationship X Y non-linear, model linear; squared predictor \\(X\\) coefficients \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\) squared, cubed, anything like (\"power one\").middle panel, captured S-shaped ('sigmoid') function using linear model. , Y variable represents probability event, probability passing exam based number hours spent studying. case, modeling relationship X Y estimating linear model special transformed space X-Y relationship linear projecting model back onto probability space (using 'link function') . non-linearity comes 'link function', model linear.Finally, right panel shows somewhat arbitrary wiggly pattern captured Generalized Additive Mixed Model—advanced technique learn course, one still, fundamentally, linear model, , just weighted sum complicated things (case, \"basis functions\"), coefficients providing weights.linear model model linear coefficients; coefficient (\\(\\beta_0\\), \\(\\beta_1\\)) model allowed set power one, term \\(\\beta_i X_j\\) involves single coefficient. terms can added terms involving coefficients multiplied divided (e.g., \\(Y = \\frac{\\beta_1}{\\beta_2} X\\) prohibited).One limitation current course focuses mainly univariate data, single response variable focus analysis. often case dealing multiple response variables subjects, modeling simultaneously technically difficult beyond scope course. simpler approach (one adopted ) analyze response variable separate univariate analysis.","code":"\nlibrary(\"tidyverse\") # if needed\n\nset.seed(62)\n\ndat <- tibble(X = runif(100, -3, 3),\n              Y = 3 + 2 * X + rnorm(100))\n\nggplot(dat, aes(X, Y)) +\n  geom_point() +\n  geom_abline(intercept = 3, slope = 2, color = \"blue\")"},{"path":"introduction.html","id":"mixed-models","chapter":"1 Introduction","heading":"1.2.2 Mixed models","text":"generalizability inference interpretation study result refers degree can readily applied situations beyond particular study context (subjects, stimuli, task, etc.). best, findings apply members human species across wide variety stimuli tasks; worst, apply specific people encountering specific stimuli used, observed specific context study.generalizability finding depends several factors: study designed, materials used, subjects recruited, nature task given subjects, way data analyzed. last point focus course. analyzing dataset, want make generalizable claims, must make decisions count replication study—aspects remain fixed across replications, allow vary.Unfortunately, sometimes find data analyzed way support generalization broadest sense, often analyses underestimate influence ideosyncratic features stimulus materials experimental task observed result (Yarkoni, 2019).","code":""},{"path":"introduction.html","id":"a-note-on-reproducibility","chapter":"1 Introduction","heading":"1.3 A note on reproducibility","text":"approach data analysis course write scripts R.term reproducibility refers degree possible reproduce pattern findings study various circumstances.say finding analytically computationally reproducible , given raw data, can obtain pattern results. Note different saying finding replicable, refers able reproduce finding new samples. widespread agreement terms, convenient view analytic reproducibility replicability two different related types reproducibility, former capturing reproducibility across analysts (among analysts time) latter reflecting reproducibility across participant samples subpopulations.Ensuring analyses reproducible hard problem. fail properly document analyses, software used gets modified goes date becomes unavailable, may trouble reproducing findings!Another important property analyses transparency: extent steps research study made available. study may transparent reproducible, vice versa. important use workflow promotes transparency. makes 'edit--execute' workflow script programming ideal data analysis, far superior 'point--click' workflow commercial statistical programs. writing code, make logic decision process explicit others easy reconstruct.","code":""},{"path":"introduction.html","id":"a-simulation-based-approach","chapter":"1 Introduction","heading":"1.4 A simulation-based approach","text":"final important characteristic course uses simulation-based approach learning statistical models. data simulation mean specifying model characterize population interest using computer's random number generator simulate process sampling data population. look simple example .typical problem face analyze data know 'ground truth' population studying. take sample population, make assumptions observed data generated, use observed data estimate unknown population parameters uncertainty around parameters.Data simulation inverts process. define parameters model representing ground truth (hypothetical) population generate data . can analyze resulting data way normally , see well parameter estimates correspond true values.look example. assume interested question whether parent toddler 'sharpens' reflexes. ever taken care toddler, know physical danger always seems imminent—fall chair just climbed , slam finger door, bang head corner table, etc.—need attentive ready act fast. hypothesize vigilance translate faster response times situations toddler around, psychological laboratory. recruit set parents toddlers come lab. give parent task pressing button quickly possible response flashing light, measure response time (milliseconds). parent, calculate mean response time trials. can simulate mean response time say, 50 parents using rnorm() function R. , load packages need (tidyverse) set random seed make sure (reader) get random values (author).chose generate data using rnorm()—function generates random numbers normal distribution—reflecting assumption mean response time normally distributed population. normal distribution defined two parameters, mean (usually notated Greek letter \\(\\mu\\), pronounced \"myoo\"), standard deviation (usually notated Greek letter \\(\\sigma\\), pronounced \"sigma\"). Since generated data , \\(\\mu\\) \\(\\sigma\\) known, call rnorm(), set values 480 40 respectively.course, test hypothesis, need comparison group, define control group non-parents. generate data control group way , changing mean.put tibble make easier plot analyze data. row table represents mean response time particular subject.things try simulated data.Plot data sensible way.Calculate means standard deviations. compare population parameters?Run t-test data. effect group significant?done things, , changing sample size, population parameters .","code":"\nlibrary(\"tidyverse\")\n\nset.seed(2021)  # can be any arbitrary integer\nparents <- rnorm(n = 50, mean = 480, sd = 40)##  [1] 475.1016 502.0983 493.9460 494.3853 515.9221 403.0972 490.4698 516.6227\n##  [9] 480.5509 549.1985 436.7118 469.0870 487.2798 540.3417 544.1788 406.3410\n## [17] 544.9324 485.2556 539.2449 540.5327 442.3023 472.5726 435.9550 528.3246\n## [25] 415.0025 484.2151 421.7823 465.8394 476.2520 524.0267 401.4470 422.0822\n## [33] 520.7777 423.1433 455.8187 416.6610 428.5627 421.8126 476.5172 500.1895\n## [41] 484.6555 550.4085 466.1953 564.8000 478.6249 448.3138 539.0206 450.9777\n## [49] 492.4952 507.6786\ncontrol <- rnorm(n = 50, mean = 500, sd = 40)\ndat <- tibble(group = rep(c(\"parent\", \"control\"), each = 50),\n              rt = c(parents, control))\n\ndat## # A tibble: 100 × 2\n##    group     rt\n##    <chr>  <dbl>\n##  1 parent  475.\n##  2 parent  502.\n##  3 parent  494.\n##  4 parent  494.\n##  5 parent  516.\n##  6 parent  403.\n##  7 parent  490.\n##  8 parent  517.\n##  9 parent  481.\n## 10 parent  549.\n## # ℹ 90 more rows"},{"path":"correlation-and-regression.html","id":"correlation-and-regression","chapter":"2 Correlation and regression","heading":"2 Correlation and regression","text":"","code":""},{"path":"correlation-and-regression.html","id":"correlation-matrices","chapter":"2 Correlation and regression","heading":"2.1 Correlation matrices","text":"may familiar concept correlation matrix reading papers psychology. Correlation matrices common way summarizing relationships multiple measurements taken individual.say measured psychological well-using multiple scales. One question extent scales measuring thing. Often look correlation matrix explore pairwise relationships measures.Recall correlation coefficient quantifies strength direction relationship two variables. usually represented symbol \\(r\\) \\(\\rho\\) (Greek letter \"rho\"). correlation coefficient ranges -1 1, 0 corresponding relationship, positive values reflecting positive relationship (one variable increases, ), negative values reflecting negative relationship (one variable increases, decreases).\nFigure 2.1: Different types bivariate relationships.\n\\(n\\) measures, many pairwise correlations can compute? can figure either formula info box , easily can computed directly choose(n, 2) function R. instance, get number possible pairwise correlations 6 measures, type choose(6, 2), tells 15 combinations.\n\\(n\\) measures, can calculate \\(\\frac{n!}{2(n - 2)!}\\) pairwise correlations measures. \\(!\\) symbol called factorial operator, defined product numbers 1 \\(n\\). , six measurements, \n\n\\[\n\\frac{6!}{2(6-2)!} = \\frac{1 \\times 2 \\times 3 \\times 4 \\times 5 \\times 6}{2\\left(1 \\times 2 \\times 3 \\times 4\\right)} = \\frac{720}{2(24)} = 15\n\\]\ncan create correlation matrix R using base::cor() corrr::correlate(). prefer latter function cor() requires data stored matrix, whereas data working tabular data stored data frame. corrr::correlate() function takes data frame first argument, provides \"tidy\" output, integrates better tidyverse functions pipes (%>%).create correlation matrix see works. Start loading packages need.use starwars dataset, built-dataset becomes available load tidyverse package. dataset information various characters appeared Star Wars film series. look correlation betweenYou can look bivariate correlation intersection given row column. correlation height mass .134, can find row 1, column 2 row 2, column 1; values . Note choose(3, 2) = 3 unique bivariate relationships, appears twice table. might want show unique pairs. can appending corrr::shave() pipeline.Now got lower triangle correlation matrix, NA values ugly leading zeroes. corrr package also provides fashion() function cleans things (see ?corrr::fashion options).Correlations provide good description relationship relationship (roughly) linear severe outliers wielding strong influence results. always good idea visualize correlations well quantify . base::pairs() function . first argument pairs() simply form ~ v1 + v2 + v3 + ... + vn v1, v2, etc. names variables want correlate.\nFigure 2.2: Pairwise correlations starwars dataset\ncan see big outlier influencing data; particular, creature mass greater 1200kg! find eliminate dataset.OK, see data look without massive creature.\nFigure 2.3: Pairwise correlations starwars dataset removing outlying mass value.\nBetter, creature outlying birth year might want get rid .Yoda. old universe. drop see plots look.\nFigure 2.4: Pairwise correlations starwars dataset removing outlying mass birth_year values.\nlooks much better. see changes correlation matrix.Note values quite different ones started .Sometimes great idea remove outliers. Another approach dealing outliers use robust method. default correlation coefficient computed corrr::correlate() Pearson product-moment correlation coefficient. can also compute Spearman correlation coefficient changing method() argument correlate(). replaces values ranks computing correlation, outliers still included, dramatically less influence.Incidentally, generating report R Markdown want tables nicely formatted can use knitr::kable().","code":"\nlibrary(\"tidyverse\")\nlibrary(\"corrr\")  # install.packages(\"corrr\") in console if missing\nstarwars %>%\n  select(height, mass, birth_year) %>%\n  correlate()## Correlation computed with\n## • Method: 'pearson'\n## • Missing treated using: 'pairwise.complete.obs'## # A tibble: 3 × 4\n##   term       height   mass birth_year\n##   <chr>       <dbl>  <dbl>      <dbl>\n## 1 height     NA      0.131     -0.404\n## 2 mass        0.131 NA          0.478\n## 3 birth_year -0.404  0.478     NA\nstarwars %>%\n  select(height, mass, birth_year) %>%\n  correlate() %>%\n  shave()## Correlation computed with\n## • Method: 'pearson'\n## • Missing treated using: 'pairwise.complete.obs'## # A tibble: 3 × 4\n##   term       height   mass birth_year\n##   <chr>       <dbl>  <dbl>      <dbl>\n## 1 height     NA     NA             NA\n## 2 mass        0.131 NA             NA\n## 3 birth_year -0.404  0.478         NA\nstarwars %>%\n  select(height, mass, birth_year) %>%\n  correlate() %>%\n  shave() %>%\n  fashion()## Correlation computed with\n## • Method: 'pearson'\n## • Missing treated using: 'pairwise.complete.obs'##         term height mass birth_year\n## 1     height                       \n## 2       mass    .13                \n## 3 birth_year   -.40  .48\npairs(~ height + mass + birth_year, starwars)\nstarwars %>%\n  filter(mass > 1200) %>%\n  select(name, mass, height, birth_year)## # A tibble: 1 × 4\n##   name                   mass height birth_year\n##   <chr>                 <dbl>  <int>      <dbl>\n## 1 Jabba Desilijic Tiure  1358    175        600\nstarwars2 <- starwars %>%\n  filter(name != \"Jabba Desilijic Tiure\")\n\npairs(~height + mass + birth_year, starwars2)\nstarwars2 %>%\n  filter(birth_year > 800) %>%\n  select(name, height, mass, birth_year)## # A tibble: 1 × 4\n##   name  height  mass birth_year\n##   <chr>  <int> <dbl>      <dbl>\n## 1 Yoda      66    17        896\nstarwars3 <- starwars2 %>%\n  filter(name != \"Yoda\")\n\npairs(~height + mass + birth_year, starwars3)\nstarwars3 %>%\n  select(height, mass, birth_year) %>%\n  correlate() %>%\n  shave() %>%\n  fashion()## Correlation computed with\n## • Method: 'pearson'\n## • Missing treated using: 'pairwise.complete.obs'##         term height mass birth_year\n## 1     height                       \n## 2       mass    .73                \n## 3 birth_year    .44  .24\nstarwars %>%\n  select(height, mass, birth_year) %>%\n  correlate(method = \"spearman\") %>%\n  shave() %>%\n  fashion()## Correlation computed with\n## • Method: 'spearman'\n## • Missing treated using: 'pairwise.complete.obs'##         term height mass birth_year\n## 1     height                       \n## 2       mass    .72                \n## 3 birth_year    .15  .15\nstarwars %>%\n  select(height, mass, birth_year) %>%\n  correlate(method = \"spearman\") %>%\n  shave() %>%\n  fashion() %>%\n  knitr::kable()"},{"path":"correlation-and-regression.html","id":"simulating-bivariate-data","chapter":"2 Correlation and regression","heading":"2.2 Simulating bivariate data","text":"already learned simulate data normal distribution using function rnorm(). Recall rnorm() allows specify mean standard deviation single variable. simulate correlated variables?clear just run rnorm() twice combine variables, end two variables unrelated, .e., correlation zero.package MASS provides function mvrnorm() 'multivariate' version rnorm (hence function name, mv + rnorm, makes easy remember.\nMASS package comes pre-installed R. function ’ll probably ever want use MASS mvrnorm(), rather load package using library(\"MASS\"), preferable use MASS::mvrnorm(), especially MASS dplyr package tidyverse don’t play well together, due packages function select(). load MASS load tidyverse, ’ll end getting MASS version select() instead dplyr version. head trying figure wrong code, always use MASS::mvrnorm() without loading library(\"MASS\").\n\nMASS dplyr, clashes dire;dplyr MASS, pain ass. #rstats pic.twitter.com/vHIbGwSKd8\nlook documentation mvrnorm() function (type ?MASS::mvrnorm console).three arguments take note :descriptions n mu understandable, \"positive-definite symmetric matrix specifying covariance\"?multivariate data, covariance matrix (also known variance-covariance matrix) specifies variances individual variables interrelationships. like multidimensional version standard deviation. fully describe univariate normal distribution, need know mean standard deviation; describe bivariate normal distribution, need means two variables, standard deviations, correlation; multivariate distribution two variables need means variables, standard deviations, possible pairwise correlations. concepts become important start talking mixed-effects modelling.can think covariance matrix something like correlation matrix saw ; indeed, calculations can turn covariance matrix correlation matrix.talk Matrix? sci-fi film series 1990s?mathematics, matrices just generalizations concept vector: vector can thought one dimension, whereas matrix can number dimensions.matrix\\[\n\\begin{pmatrix}\n1 & 4 & 7 \\\\\n2 & 5 & 8 \\\\\n3 & 6 & 9 \\\\\n\\end{pmatrix}\n\\]3 (row) 3 (column) matrix containing column vectors \\(\\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\\\ \\end{pmatrix}\\), \\(\\begin{pmatrix} 4 \\\\ 5 \\\\ 6 \\\\ \\end{pmatrix}\\), \\(\\begin{pmatrix} 7 \\\\ 8 \\\\ 9 \\\\ \\end{pmatrix}\\). Conventionally, refer matrices \\(\\) \\(j\\) format, \\(\\) number rows \\(j\\) number columns. 3x2 matrix 3 rows 2 columns, like .\\[\n\\begin{pmatrix}\n& d \\\\\nb & e \\\\\nc & f \\\\\n\\end{pmatrix}\n\\]square matrix matrix number rows equal number columns.can create matrix R using matrix() function (see ) binding together vectors using base R cbind() rbind(), bind vectors together column-wise row-wise, respectively. Try cbind(1:3, 4:6, 7:9) console.Now matrix \"positive-definite\" \"symmetric\"? mathematical requirements kinds matrices can represent possible multivariate normal distributions. words, covariance matrix supply must represent legal multivariate normal distribution. point, really need know much .start simulating data representing hypothetical humans heights weights. know things correlated. need able simulate data means standard deviations two variables correlation.found data converted CSV file. want follow along, download file heights_and_weights.csv. scatterplot looks:\nFigure 2.5: Heights weights 475 humans (including infants)\nNow, quite linear relationship. can make one log transforming variables first.\nFigure 2.6: Log transformed heights weights.\nfact big cluster points top right tail cloud probably indicates adults kids sample, since adults taller heavier.mean log height 4.11 (SD = 0.26), mean log weight 4.74 (SD = 0.65). correlation log height log weight, can get using cor() function, high, 0.96.now information need simulate heights weights , say, 500 humans. get information MASS::mvrnorm()? know first part function call MASS::mvrnorm(500, c(4.11, 4.74), ...), Sigma, covariance matrix? know \\(\\hat{\\sigma}_x = 0.26\\) \\(\\hat{\\sigma}_y = 0.65\\), \\(\\hat{\\rho}_{xy} = 0.96\\).covariance matrix representating Sigma (\\(\\mathbf{\\Sigma}\\)) bivariate data following format:\\[\n\\mathbf{\\Sigma} =\n\\begin{pmatrix}\n{\\sigma_x}^2                & \\rho_{xy} \\sigma_x \\sigma_y \\\\\n\\rho_{yx} \\sigma_y \\sigma_x & {\\sigma_y}^2 \\\\\n\\end{pmatrix}\n\\]variances (squared standard deviations, \\({\\sigma_x}^2\\) \\({\\sigma_y}^2\\)) diagonal, covariances (correlation times two standard deviations, \\(\\rho_{xy} \\sigma_x \\sigma_y\\)) -diagonal. useful remember covariance just correlation times product two standard deviations. saw correlation matrices, redundant information table; namely, covariance appears top right cell well bottom left cell matrix.plugging values got , covariance matrix \\[\n\\begin{pmatrix}\n.26^2 & (.96)(.26)(.65) \\\\\n(.96)(.65)(.26) & .65^2 \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\n.067 & .162 \\\\\n.162 & .423 \\\\\n\\end{pmatrix}\n\\]OK, form Sigma R can pass mvrnorm() function? use matrix() function, shown .First define covariance store variable my_cov.Now use matrix() define Sigma, my_Sigma.\nConfused matrix() function?\n\nfirst argument vector values, created using c(). ncol argument specifies many columns matrix . also nrow argument use, give one dimensions, can infer size using length vector supplied first argument.\n\ncan see matrix() fills elements matrix column column, rather row row running following code:\n\nmatrix(c(\"\", \"b\", \"c\", \"d\"), ncol = 2)\n\nwant change behavior, set byrow argument TRUE.\n\nmatrix(c(\"\", \"b\", \"c\", \"d\"), ncol = 2, byrow = TRUE)\nGreat. Now got my_Sigma, ready use MASS::mvrnorm(). test creating 6 synthetic humans.MASS::mvrnorm() returns matrix row simulated human, first column representing log height second column representing log weight. log heights log weights useful us, transform back using exp(), inverse log() transform.first simulated human 70.4 inches tall (5'5\" X) weighs 196.94 pounds (89.32 kg). Sounds right! (Note also generate observations outside original data; get super tall humans, like observation 5, least weight/height relationship preserved.)OK, randomly generate bunch humans, transform log inches pounds, plot original data see .\nFigure 2.7: Real simulated humans.\ncan see simulated humans much like normal ones, except creating humans outside normal range heights weights.","code":"\nhandw <- read_csv(\"data/heights_and_weights.csv\", col_types = \"dd\")\n\nggplot(handw, aes(height_in, weight_lbs)) + \n  geom_point(alpha = .2) +\n  labs(x = \"height (inches)\", y = \"weight (pounds)\") \nhandw_log <- handw %>%\n  mutate(hlog = log(height_in),\n         wlog = log(weight_lbs))\nmy_cov <- .96 * .26 * .65\nmy_Sigma <- matrix(c(.26^2, my_cov, my_cov, .65^2), ncol = 2)\nmy_Sigma##         [,1]    [,2]\n## [1,] 0.06760 0.16224\n## [2,] 0.16224 0.42250\nset.seed(62) # for reproducibility\n\n# passing the *named* vector c(height = 4.11, weight = 4.74)\n# for mu gives us column names in the output\nlog_ht_wt <- MASS::mvrnorm(6, \n                           c(height = 4.11, weight = 4.74), \n                           my_Sigma)\n\nlog_ht_wt##        height   weight\n## [1,] 4.254209 5.282913\n## [2,] 4.257828 4.895222\n## [3,] 3.722376 3.759767\n## [4,] 4.191287 4.764229\n## [5,] 4.739967 6.185191\n## [6,] 4.058105 4.806485\nexp(log_ht_wt)##         height    weight\n## [1,]  70.40108 196.94276\n## [2,]  70.65632 133.64963\n## [3,]  41.36254  42.93844\n## [4,]  66.10779 117.24065\n## [5,] 114.43045 485.50576\n## [6,]  57.86453 122.30092\n## simulate new humans\nnew_humans <- MASS::mvrnorm(500, \n                            c(height_in = 4.11, weight_lbs = 4.74),\n                            my_Sigma) %>%\n  exp() %>% # back-transform from log to inches and pounds\n  as_tibble() %>% # make tibble for plotting\n  mutate(type = \"simulated\") # tag them as simulated\n\n## combine real and simulated datasets\n## handw is variable containing data from heights_and_weights.csv\nalldata <- bind_rows(handw %>% mutate(type = \"real\"), \n                     new_humans)\n\nggplot(alldata, aes(height_in, weight_lbs)) +\n  geom_point(aes(colour = type), alpha = .1)"},{"path":"correlation-and-regression.html","id":"relationship-between-correlation-and-regression","chapter":"2 Correlation and regression","heading":"2.3 Relationship between correlation and regression","text":"OK, know estimate correlations, wanted predict weight someone given height? might sound like impractical problem, fact, emergency medical technicians can use technique get quick estimate people's weights emergency situations need administer drugs procedures whose safety depends patient's weight, time weigh .Recall GLM simple regression model \\[Y_i = \\beta_0 + \\beta_1 X_i + e_i.\\], trying predict weight (\\(Y_i\\)) person \\(\\) observed height (\\(X_i\\)). equation, \\(\\beta_0\\) \\(\\beta_1\\) y-intercept slope parameters respectively, \\(e_i\\)s residuals. conventionally assumed \\(e_i\\) values normal distribution mean zero variance \\(\\sigma^2\\); math-y way saying \\(e_i \\sim N(0, \\sigma^2)\\), \\(\\sim\\) read \"distributed according \" \\(N(0, \\sigma^2)\\) means \"Normal distribution (\\(N\\)) mean 0 variance \\(\\sigma^2\\)\".turns estimates means X Y (denoted \\(\\mu_x\\) \\(\\mu_y\\) respectively), standard deviations (\\(\\hat{\\sigma}_x\\) \\(\\hat{\\sigma}_y\\)), correlation X Y (\\(\\hat{\\rho}\\)), information need estimate parameters regression equation \\(\\beta_0\\) \\(\\beta_1\\). .First, slope regression line \\(\\beta_1\\) equals correlation coefficient \\(\\rho\\) times ratio standard deviations \\(Y\\) \\(X\\).\\[\\beta_1 = \\rho \\frac{\\sigma_Y}{\\sigma_X}\\]Given estimates log height weight, can solve \\(\\beta_1\\)?next thing note mathematical reasons, regression line guaranteed go point corresponding mean \\(X\\) mean \\(Y\\), .e., point \\((\\mu_x, \\mu_y)\\). (can think regression line \"pivoting\" around point depending slope). also know \\(\\beta_0\\) y-intercept, point line crosses vertical axis \\(X = 0\\). information, estimates , can figure value \\(\\beta_0\\)?reasoning can solve \\(\\beta_0\\).\\(\\beta_1\\) value tells change \\(X\\) corresponding change 2.4 \\(Y\\), know line goes points \\((\\mu_x, \\mu_y)\\) well y-intercept \\((0, \\beta_0)\\).Think stepping back unit--unit \\(X = \\mu_x\\) \\(X = 0\\).\n\\(X = \\mu_x\\), \\(Y = 4.74\\). unit step take backward X dimension, \\(Y\\) drop \\(\\beta_1 = 2.4\\) units. get zero, \\(Y\\) dropped \\(\\mu_y\\) \\(\\mu_y - \\mu_x\\beta_1\\).general solution : \\(\\beta_0 = \\mu_y - \\mu_x\\beta_1\\).Since \\(\\beta_1 = 2.4\\), \\(\\mu_x = 4.11\\), \\(\\mu_y = 4.74\\), \\(\\beta_0 = -5.124\\). Thus, regression equation :\\[Y_i =  -5.124 + 2.4X_i + e_i.\\]check results, first run regression log-transformed data using lm(), estimates parameters using ordinary least squares regression.Looks pretty close. reason match exactly rounded estimates two decimal places convenience.another check, superimpose regression line computed hand scatterplot log-transformed data.\nFigure 2.8: Log transformed values superimposed regression line.\nLooks right.close, implications relationship correlation regression.\\(\\beta_1 = 0\\) \\(\\rho = 0\\).\\(\\beta_1 > 0\\) implies \\(\\rho > 0\\), since standard deviations negative.\\(\\beta_1 < 0\\) implies \\(\\rho < 0\\), reason.Rejecting null hypothesis \\(\\beta_1 = 0\\) rejecting null hypothesis \\(\\rho = 0\\). p-values get \\(\\beta_1\\) lm() one get \\(\\rho\\) cor.test().","code":"\nb1 <- .96 * (.65 / .26)\nb1## [1] 2.4\nsummary(lm(wlog ~ hlog, handw_log))## \n## Call:\n## lm(formula = wlog ~ hlog, data = handw_log)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.63296 -0.09915 -0.01366  0.09285  0.65635 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -5.26977    0.13169  -40.02   <2e-16 ***\n## hlog         2.43304    0.03194   76.17   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.1774 on 473 degrees of freedom\n## Multiple R-squared:  0.9246, Adjusted R-squared:  0.9245 \n## F-statistic:  5802 on 1 and 473 DF,  p-value: < 2.2e-16\nggplot(handw_log, aes(hlog, wlog)) +\n  geom_point(alpha = .2) +\n  labs(x = \"log(height)\", y = \"log(weight)\") +\n  geom_abline(intercept = -5.124, slope = 2.4, colour = 'blue')"},{"path":"correlation-and-regression.html","id":"exercises","chapter":"2 Correlation and regression","heading":"2.4 Exercises","text":"","code":""},{"path":"multiple-regression.html","id":"multiple-regression","chapter":"3 Multiple regression","heading":"3 Multiple regression","text":"general model single-level data \\(m\\) predictors \\[\nY_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\ldots + \\beta_m X_{mi} + e_i\n\\]\\(e_i \\sim \\mathcal{N}\\left(0, \\sigma^2\\right)\\)—words, assumption errors normal distribution mean zero variance \\(\\sigma^2\\).Note key assumption response variable (\\(Y\\)s) normally distributed, individual predictor variables (\\(X\\)s) normally distributed; model residuals normally distributed (discussion, see blog post). individual \\(X\\) predictor variables can combination continuous /categorical predictors, including interactions among variables. assumptions behind particular model relationship \"planar\" (can described flat surface, analogous linearity assumption simple regression) error variance independent predictor values.\\(\\beta\\) values referred regression coefficients. \\(\\beta_h\\) interpreted partial effect \\(\\beta_h\\) holding constant predictor variables. \\(m\\) predictor variables, \\(m+1\\) regression coefficients: one intercept, one predictor.Although discussions multiple regression common statistical textbooks, rarely able apply exact model . model assumes single-level data, whereas psychological data multi-level. However, fundamentals types datasets, worthwhile learning simpler case first.","code":""},{"path":"multiple-regression.html","id":"an-example-how-to-get-a-good-grade-in-statistics","chapter":"3 Multiple regression","heading":"3.1 An example: How to get a good grade in statistics","text":"look (made , realistic) data see can use multiple regression answer various study questions. hypothetical study, dataset 100 statistics students, includes final course grade (grade), number lectures student attended (lecture, integer ranging 0-10), many times student clicked download online materials (nclicks) student's grade point average prior taking course, GPA, ranges 0 (fail) 4 (highest possible grade).","code":""},{"path":"multiple-regression.html","id":"data-import-and-visualization","chapter":"3 Multiple regression","heading":"3.1.1 Data import and visualization","text":"load data grades.csv look.First look pairwise correlations.\nFigure 2.2: pairwise relationships grades dataset.\n","code":"\nlibrary(\"corrr\") # correlation matrices\nlibrary(\"tidyverse\")\n\ngrades <- read_csv(\"data/grades.csv\", col_types = \"ddii\")\n\ngrades## # A tibble: 100 × 4\n##    grade   GPA lecture nclicks\n##    <dbl> <dbl>   <int>   <int>\n##  1  2.40 1.13        6      88\n##  2  3.67 0.971       6      96\n##  3  2.85 3.34        6     123\n##  4  1.36 2.76        9      99\n##  5  2.31 1.02        4      66\n##  6  2.58 0.841       8      99\n##  7  2.69 4           5      86\n##  8  3.05 2.29        7     118\n##  9  3.21 3.39        9      98\n## 10  2.24 3.27       10     115\n## # ℹ 90 more rows\ngrades %>%\n  correlate() %>%\n  shave() %>%\n  fashion()## Correlation computed with\n## • Method: 'pearson'\n## • Missing treated using: 'pairwise.complete.obs'##      term grade  GPA lecture nclicks\n## 1   grade                           \n## 2     GPA   .25                     \n## 3 lecture   .24  .44                \n## 4 nclicks   .16  .30     .36\npairs(grades)"},{"path":"multiple-regression.html","id":"estimation-and-interpretation","chapter":"3 Multiple regression","heading":"3.1.2 Estimation and interpretation","text":"estimate regression coefficients (\\(\\beta\\)s), use lm() function. GLM \\(m\\) predictors:\\[\nY_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\ldots + \\beta_m X_{mi} + e_i\n\\]call base R's lm() islm(Y ~ X1 + X2 + ... + Xm, data)Y variable response variable, X variables predictor variables. Note need explicitly specify intercept residual terms; included default.current data, predict grade lecture nclicks.often write parameter symbol little hat top make clear dealing estimates sample rather (unknown) true population values. :\\(\\hat{\\beta}_0\\) = 1.46\\(\\hat{\\beta}_1\\) = 0.09\\(\\hat{\\beta}_2\\) = 0.01This tells us person's predicted grade related lecture attendance download rate following formula:grade = 1.46 + 0.09 \\(\\times\\) lecture + 0.01 \\(\\times\\) nclicksBecause \\(\\hat{\\beta}_1\\) \\(\\hat{\\beta}_2\\) positive, know higher values lecture nclicks associated higher grades.asked, grade predict student attends 3 lectures downloaded 70 times, easily figure substituting appropriate values.grade = 1.46 + 0.09 \\(\\times\\) 3 + 0.01 \\(\\times\\) 70which equalsgrade = 1.46 + 0.27 + 0.7and reduces tograde = 2.43","code":"\nmy_model <- lm(grade ~ lecture + nclicks, grades)\n\nsummary(my_model)## \n## Call:\n## lm(formula = grade ~ lecture + nclicks, data = grades)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -2.21653 -0.40603  0.02267  0.60720  1.38558 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)  \n## (Intercept) 1.462037   0.571124   2.560   0.0120 *\n## lecture     0.091501   0.045766   1.999   0.0484 *\n## nclicks     0.005052   0.006051   0.835   0.4058  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.8692 on 97 degrees of freedom\n## Multiple R-squared:  0.06543,    Adjusted R-squared:  0.04616 \n## F-statistic: 3.395 on 2 and 97 DF,  p-value: 0.03756"},{"path":"multiple-regression.html","id":"predictions-from-the-linear-model-using-predict","chapter":"3 Multiple regression","heading":"3.1.3 Predictions from the linear model using predict()","text":"want predict response values new predictor values, can use predict() function base R.predict() takes two main arguments. first argument fitted model object (.e., my_model ) second data frame (tibble) containing new values predictors.\nneed include predictor variables new table. ’ll get error message tibble missing predictors. also need make sure variable names new table exactly match variable names model.\ncreate tibble new values try .tribble() function provides way build tibble row row, whereas tibble() table built column column.first row tribble() contains column names, preceded tilde (~).sometimes easier read row row, although result . Consider made table usingNow created table new_data, just pass predict() return vector predictions \\(Y\\) (grade).great, maybe want line predictor values. can just adding new column new_data.Want see options predict()? Check help ?predict.lm.","code":"\n## a 'tribble' is a way to make a tibble by rows,\n## rather than by columns. This is sometimes useful\nnew_data <- tribble(~lecture, ~nclicks,\n                    3, 70,\n                    10, 130,\n                    0, 20,\n                    5, 100)\nnew_data <- tibble(lecture = c(3, 10, 0, 5),\n                   nclicks = c(70, 130, 20, 100))\npredict(my_model, new_data)##        1        2        3        4 \n## 2.090214 3.033869 1.563087 2.424790\nnew_data %>%\n  mutate(predicted_grade = predict(my_model, new_data))## # A tibble: 4 × 3\n##   lecture nclicks predicted_grade\n##     <dbl>   <dbl>           <dbl>\n## 1       3      70            2.09\n## 2      10     130            3.03\n## 3       0      20            1.56\n## 4       5     100            2.42"},{"path":"multiple-regression.html","id":"visualizing-partial-effects","chapter":"3 Multiple regression","heading":"3.1.4 Visualizing partial effects","text":"noted parameter estimates regression coefficient tell us partial effect variable; effect holding others constant. way visualize partial effect? Yes, can using predict() function, making table varying values focal predictor, filling predictors mean values.example, visualize partial effect lecture grade holding nclicks constant mean value.Now plot.\nFigure 3.1: Partial effect 'lecture' grade, nclicks mean value.\nPartial effect plots make sense interactions model focal predictor predictor.reason interactions, partial effect focal predictor \\(X_i\\) differ across values variables interacts .Now can visualize partial effect nclicks grade?See solution bottom page.","code":"\nnclicks_mean <- grades %>% pull(nclicks) %>% mean()\n\n## new data for prediction\nnew_lecture <- tibble(lecture = 0:10,\n                      nclicks = nclicks_mean)\n\n## add the predicted value to new_lecture\nnew_lecture2 <- new_lecture %>%\n  mutate(grade = predict(my_model, new_lecture))\n\nnew_lecture2## # A tibble: 11 × 3\n##    lecture nclicks grade\n##      <int>   <dbl> <dbl>\n##  1       0    98.3  1.96\n##  2       1    98.3  2.05\n##  3       2    98.3  2.14\n##  4       3    98.3  2.23\n##  5       4    98.3  2.32\n##  6       5    98.3  2.42\n##  7       6    98.3  2.51\n##  8       7    98.3  2.60\n##  9       8    98.3  2.69\n## 10       9    98.3  2.78\n## 11      10    98.3  2.87\nggplot(grades, aes(lecture, grade)) + \n  geom_point() +\n  geom_line(data = new_lecture2)"},{"path":"multiple-regression.html","id":"standardizing-coefficients","chapter":"3 Multiple regression","heading":"3.1.5 Standardizing coefficients","text":"One kind question often use multiple regression address , predictors matter predicting Y?Now, just read \\(\\hat{\\beta}\\) values choose one largest absolute value, predictors different scales. answer question, need center scale predictors.Remember \\(z\\) scores?\\[\nz = \\frac{X - \\mu_x}{\\sigma_x}\n\\]\\(z\\) score represents distance score \\(X\\) sample mean (\\(\\mu_x\\)) standard deviation units (\\(\\sigma_x\\)). \\(z\\) score 1 means score one standard deviation mean; \\(z\\)-score -2.5 means 2.5 standard deviations mean. \\(Z\\)-scores give us way comparing things come different populations calibrating standard normal distribution (distribution mean 0 standard deviation 1).re-scale predictors converting \\(z\\)-scores. easy enough .Now re-fit model using centered scaled predictors.tells us lecture_c relatively larger influence; standard deviation increase variable, grade increases 0.19.Another common approach standardization involves standardizing response variable well predictors, .e., \\(z\\)-scoring \\(Y\\) values well \\(X\\) values. relative rank order regression coefficients approach. main difference coefficients expressed standard deviation (\\(SD\\)) units response variable, rather raw units.Multicollinearity discontentsIn discussions multiple regression may hear concerns expressed \"multicollinearity\", fancy way referring existence intercorrelations among predictor variables. potential problem insofar potentially affects interpretation effects individual predictor variables. predictor variables correlated, \\(\\beta\\) values can change depending upon predictors included excluded model, sometimes even changing signs. key things keep mind :correlated predictors probably unavoidable observational studies;regression assume predictor variables independent one another (words, finding correlations amongst predictors reason question model);strong correlations present, use caution interpreting individual regression coefficients;known \"remedy\" , clear remedy desireable, many -called remedies harm good.information guidance, see (Vanhove, 2021).","code":"\ngrades2 <- grades %>%\n  mutate(lecture_c = (lecture - mean(lecture)) / sd(lecture),\n         nclicks_c = (nclicks - mean(nclicks)) / sd(nclicks))\n\ngrades2## # A tibble: 100 × 6\n##    grade   GPA lecture nclicks lecture_c nclicks_c\n##    <dbl> <dbl>   <int>   <int>     <dbl>     <dbl>\n##  1  2.40 1.13        6      88  -0.484     -0.666 \n##  2  3.67 0.971       6      96  -0.484     -0.150 \n##  3  2.85 3.34        6     123  -0.484      1.59  \n##  4  1.36 2.76        9      99   0.982      0.0439\n##  5  2.31 1.02        4      66  -1.46      -2.09  \n##  6  2.58 0.841       8      99   0.493      0.0439\n##  7  2.69 4           5      86  -0.972     -0.796 \n##  8  3.05 2.29        7     118   0.00488    1.27  \n##  9  3.21 3.39        9      98   0.982     -0.0207\n## 10  2.24 3.27       10     115   1.47       1.08  \n## # ℹ 90 more rows\nmy_model_scaled <- lm(grade ~ lecture_c + nclicks_c, grades2)\n\nsummary(my_model_scaled)## \n## Call:\n## lm(formula = grade ~ lecture_c + nclicks_c, data = grades2)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -2.21653 -0.40603  0.02267  0.60720  1.38558 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  2.59839    0.08692  29.895   <2e-16 ***\n## lecture_c    0.18734    0.09370   1.999   0.0484 *  \n## nclicks_c    0.07823    0.09370   0.835   0.4058    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.8692 on 97 degrees of freedom\n## Multiple R-squared:  0.06543,    Adjusted R-squared:  0.04616 \n## F-statistic: 3.395 on 2 and 97 DF,  p-value: 0.03756"},{"path":"multiple-regression.html","id":"model-comparison","chapter":"3 Multiple regression","heading":"3.1.6 Model comparison","text":"Another common kind question multiple regression also used address form: predictor set predictors interest significantly impact response variable effects control variables?example, saw model including lecture nclicks statistically significant,\n\\(F(2, 97) = 3.395\\),\n\\(p = 0.038\\).null hypothesis regression model \\(m\\) predictors \\[H_0: \\beta_1 = \\beta_2 = \\ldots = \\beta_m = 0;\\]words, coefficients (except intercept) zero. null hypothesis true, null model\\[Y_i = \\beta_0\\]gives just good prediction model including predictors coefficients. words, best prediction \\(Y\\) just mean (\\(\\mu_y\\)); \\(X\\) variables irrelevant. rejected null hypothesis, implies can better including two predictors, lecture nclicks.might ask: maybe case better students get better grades, relationship lecture, nclicks, grade just mediated student quality. , better students likely go lecture download materials. can ask, attendance downloads associated better grades beyond student ability, measured GPA?way can test hypothesis using model comparison. logic follows. First, estimate model containing control predictors excluding focal predictors interest. Second, estimate model containing control predictors well focal predictors. Finally, compare two models, see statistically significant gain including predictors.:null hypothesis just good predicting grade GPA predicting GPA plus lecture nclicks. reject null adding two variables leads substantial enough reduction residual sums squares (RSS); .e., explain away enough residual variance.see case:\n\\(F(2, 96 ) = 1.308\\),\n\\(p = 0.275\\). evidence lecture attendance downloading online materials associated better grades beyond student ability, measured GPA.","code":"\nm1 <- lm(grade ~ GPA, grades) # control model\nm2 <- lm(grade ~ GPA + lecture + nclicks, grades) # bigger model\n\nanova(m1, m2)## Analysis of Variance Table\n## \n## Model 1: grade ~ GPA\n## Model 2: grade ~ GPA + lecture + nclicks\n##   Res.Df    RSS Df Sum of Sq      F Pr(>F)\n## 1     98 73.528                           \n## 2     96 71.578  2    1.9499 1.3076 0.2752"},{"path":"multiple-regression.html","id":"dealing-with-categorical-predictors","chapter":"3 Multiple regression","heading":"3.2 Dealing with categorical predictors","text":"regression formula characterizes response variable sum weighted predictors. one predictors categorical (e.g., representing groups \"rural\" \"urban\") rather numeric? Many variables nominal: categorical variable containing names, inherent ordering among levels variable. Pet ownership (cat, dog, ferret) nominal variable; preferences aside, owning cat greater owning dog, owning dog greater owning ferret.Representing nominal data using numeric predictorsRepresenting nominal variable \\(k\\) levels regression model requires \\(k-1\\) numeric predictors; instance, four levels, need three predictors. numerical coding schemes require choose one \\(k\\) levels baseline level. \\(k-1\\) variables contrasts one levels level baseline.Example: variable, pet_type three levels (cat, dog, ferret).choose cat baseline, create two numeric predictor variables:dog_v_cat encode contrast dog cat, andferret_v_cat encode contrast ferret cat.Nominal variables typically represented data frame type character factor.difference character factor variable factors contain information levels order, character vectors lack information.specify model using R formula syntax, R check data types predictors right hand side formula. example, model regresses income pet_type (e.g., income ~ pet_type), R checks data type pet_type.variable type character factor, R implicitly create numeric predictor (set predictors) represent variable model. different schemes available creating numeric representations nominal variables. default R use dummy ('treatment') coding (see ). Unfortunately, default unsuitable many types study designs psychology, going recommend learn code predictor variables \"hand,\" make habit .represent levels categorical variable numbers!example, variable pet_type levels cat, dog, ferret. Sometimes people represent levels nominal variable numbers, like :1 cat,2 dog,3 ferret.bad idea.First, labeling arbitrary opaque anyone attempting use data know number goes category (also forget!).Even worse, put variable predictor regression model, R way knowing intention use 1, 2, 3 arbitrary labels groups, instead assume pet_type measurement dogs 1 unit greater cats, ferrets 2 units greater cats 1 unit greater dogs, nonsense!far easy make mistake, difficult catch authors share data code. 2016, paper religious affiliation altruism children published Current Biology retracted just kind mistake., represent levels nominal variable numbers, except course deliberately create predictor variables encoding \\(k-1\\) contrasts needed properly represent nominal variable regression model.","code":""},{"path":"multiple-regression.html","id":"dummy-a.k.a.-treatment-coding","chapter":"3 Multiple regression","heading":"3.2.1 Dummy (a.k.a. \"treatment\") coding","text":"nominal variable two levels, choose one level baseline, create new variable 0 whenever level baseline 1 level. choice baseline arbitrary, affect whether coefficient positive negative, magnitude, standard error associated p-value.illustrate, gin fake data single two level categorical predictor.Now add new variable, group_d, dummy coded group variable. use dplyr::if_else() function define new column.Now just run regular regression model.reverse coding. get result, just sign different.interpretation intercept estimated mean group coded zero. can see plugging zero X prediction formula . Thus, \\(\\beta_1\\) can interpreted difference mean baseline group group coded 1.\\[\\hat{Y_i} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i \\]Note just put character variable group predictor model, R automatically create dummy variable (variables) us needed.lm() function examines group figures unique levels variable, case B. chooses baseline level comes first alphabetically, encodes contrast level (B) baseline level (). (case group defined factor, baseline level first element levels(fake_data$group)).new variable created shows name groupB output.","code":"\nfake_data <- tibble(Y = rnorm(10),\n                    group = rep(c(\"A\", \"B\"), each = 5))\n\nfake_data## # A tibble: 10 × 2\n##           Y group\n##       <dbl> <chr>\n##  1 -0.797   A    \n##  2  1.04    A    \n##  3 -1.03    A    \n##  4  2.21    A    \n##  5 -0.314   A    \n##  6  0.00865 B    \n##  7  1.39    B    \n##  8  0.0544  B    \n##  9  2.81    B    \n## 10 -0.727   B\nfake_data2 <- fake_data %>%\n  mutate(group_d = if_else(group == \"B\", 1, 0))\n\nfake_data2## # A tibble: 10 × 3\n##           Y group group_d\n##       <dbl> <chr>   <dbl>\n##  1 -0.797   A           0\n##  2  1.04    A           0\n##  3 -1.03    A           0\n##  4  2.21    A           0\n##  5 -0.314   A           0\n##  6  0.00865 B           1\n##  7  1.39    B           1\n##  8  0.0544  B           1\n##  9  2.81    B           1\n## 10 -0.727   B           1\nsummary(lm(Y ~ group_d, fake_data2))## \n## Call:\n## lm(formula = Y ~ group_d, data = fake_data2)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1.4352 -0.9411 -0.5960  0.7859  2.1069 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)\n## (Intercept)   0.2241     0.6208   0.361    0.727\n## group_d       0.4840     0.8779   0.551    0.596\n## \n## Residual standard error: 1.388 on 8 degrees of freedom\n## Multiple R-squared:  0.03661,    Adjusted R-squared:  -0.08382 \n## F-statistic: 0.304 on 1 and 8 DF,  p-value: 0.5965\nfake_data3 <- fake_data %>%\n  mutate(group_d = if_else(group == \"A\", 1, 0))\n\nsummary(lm(Y ~ group_d, fake_data3))## \n## Call:\n## lm(formula = Y ~ group_d, data = fake_data3)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1.4352 -0.9411 -0.5960  0.7859  2.1069 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)\n## (Intercept)   0.7081     0.6208   1.141    0.287\n## group_d      -0.4840     0.8779  -0.551    0.596\n## \n## Residual standard error: 1.388 on 8 degrees of freedom\n## Multiple R-squared:  0.03661,    Adjusted R-squared:  -0.08382 \n## F-statistic: 0.304 on 1 and 8 DF,  p-value: 0.5965\nlm(Y ~ group, fake_data) %>%\n  summary()## \n## Call:\n## lm(formula = Y ~ group, data = fake_data)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1.4352 -0.9411 -0.5960  0.7859  2.1069 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)\n## (Intercept)   0.2241     0.6208   0.361    0.727\n## groupB        0.4840     0.8779   0.551    0.596\n## \n## Residual standard error: 1.388 on 8 degrees of freedom\n## Multiple R-squared:  0.03661,    Adjusted R-squared:  -0.08382 \n## F-statistic: 0.304 on 1 and 8 DF,  p-value: 0.5965"},{"path":"multiple-regression.html","id":"dummy-coding-when-k-2","chapter":"3 Multiple regression","heading":"3.2.2 Dummy coding when \\(k > 2\\)","text":"nominal predictor variable two levels (\\(k > 2\\)), one numeric predictor longer sufficient; need \\(k-1\\) predictors. nominal predictor four levels, need define three predictors. simulate data work , season_wt, represents person's bodyweight (kg) four seasons year.Now add three predictors code variable season. Try see can figure works.Reminder: Always look dataWhenever write code potentially changes data, double check code works intended looking data. especially case hand-coding nominal variables use regression, sometimes code wrong, throw error.Consider code chunk , defined three contrasts represent nominal variable season, winter baseline.happen accidently misspelled one levels (summre summer) notice?code chunk runs, get confusing output run regression; namely, coefficent summer_v_winter NA (available).happened? look data find . use distinct find distinct combinations original variable season three variables created (see ?dplyr::distinct details).misspelling, predictor summer_v_winter 1 season == \"summer\"; instead, always zero. if_else() literally says 'set summer_v_winter 1 season == \"summre\", otherwise 0'. course, season never equal summre, summre typo. caught easily running check distinct(). Get habit create numeric predictors.closer look R's defaultsIf ever used point--click statistical software like SPSS, probably never learn coding categorical predictors. Normally, software recognizes predictor categorical , behind scenes, takes care recoding numerical predictor. R different: supply predictor type character factor linear modeling function, create numerical dummy-coded predictors , shown code ., R implicitly creates three dummy variables code four levels season, called seasonspring, seasonsummer seasonwinter. unmentioned season, fall, chosen baseline comes earliest alphabet. three predictors following values:seasonspring: 1 spring, 0 otherwise;seasonsummer: 1 summer, 0 otherwise;seasonwinter: 1 winter, 0 otherwise.seems like handy thing R us, dangers lurk relying default. learn dangers next chapter talk interactions.","code":"\nseason_wt <- tibble(season = rep(c(\"winter\", \"spring\", \"summer\", \"fall\"),\n                                 each = 5),\n                    bodyweight_kg = c(rnorm(5, 105, 3),\n                                      rnorm(5, 103, 3),\n                                      rnorm(5, 101, 3),\n                                      rnorm(5, 102.5, 3)))\n\nseason_wt## # A tibble: 20 × 2\n##    season bodyweight_kg\n##    <chr>          <dbl>\n##  1 winter         106. \n##  2 winter         105. \n##  3 winter         103. \n##  4 winter         110. \n##  5 winter         104. \n##  6 spring         100. \n##  7 spring          98.7\n##  8 spring          99.9\n##  9 spring         106. \n## 10 spring         108. \n## 11 summer         102. \n## 12 summer          94.1\n## 13 summer         104. \n## 14 summer         100. \n## 15 summer         101. \n## 16 fall            96.8\n## 17 fall            98.2\n## 18 fall            99.0\n## 19 fall           102. \n## 20 fall           106.\n## baseline value is 'winter'\nseason_wt2 <- season_wt %>%\n  mutate(spring_v_winter = if_else(season == \"spring\", 1, 0),\n         summer_v_winter = if_else(season == \"summer\", 1, 0),\n         fall_v_winter = if_else(season == \"fall\", 1, 0))\n\nseason_wt2## # A tibble: 20 × 5\n##    season bodyweight_kg spring_v_winter summer_v_winter fall_v_winter\n##    <chr>          <dbl>           <dbl>           <dbl>         <dbl>\n##  1 winter         106.                0               0             0\n##  2 winter         105.                0               0             0\n##  3 winter         103.                0               0             0\n##  4 winter         110.                0               0             0\n##  5 winter         104.                0               0             0\n##  6 spring         100.                1               0             0\n##  7 spring          98.7               1               0             0\n##  8 spring          99.9               1               0             0\n##  9 spring         106.                1               0             0\n## 10 spring         108.                1               0             0\n## 11 summer         102.                0               1             0\n## 12 summer          94.1               0               1             0\n## 13 summer         104.                0               1             0\n## 14 summer         100.                0               1             0\n## 15 summer         101.                0               1             0\n## 16 fall            96.8               0               0             1\n## 17 fall            98.2               0               0             1\n## 18 fall            99.0               0               0             1\n## 19 fall           102.                0               0             1\n## 20 fall           106.                0               0             1\nseason_wt3 <- season_wt %>%\n  mutate(spring_v_winter = if_else(season == \"spring\", 1, 0),\n         summer_v_winter = if_else(season == \"summre\", 1, 0),\n         fall_v_winter = if_else(season == \"fall\", 1, 0))\nlm(bodyweight_kg ~ spring_v_winter + summer_v_winter + fall_v_winter,\n   season_wt3)## \n## Call:\n## lm(formula = bodyweight_kg ~ spring_v_winter + summer_v_winter + \n##     fall_v_winter, data = season_wt3)\n## \n## Coefficients:\n##     (Intercept)  spring_v_winter  summer_v_winter    fall_v_winter  \n##        102.9002          -0.3195               NA          -2.5609\nseason_wt3 %>%\n  distinct(season, spring_v_winter, summer_v_winter, fall_v_winter)## # A tibble: 4 × 4\n##   season spring_v_winter summer_v_winter fall_v_winter\n##   <chr>            <dbl>           <dbl>         <dbl>\n## 1 winter               0               0             0\n## 2 spring               1               0             0\n## 3 summer               0               0             0\n## 4 fall                 0               0             1\nlm(bodyweight_kg ~ season, season_wt) %>%\n  summary()## \n## Call:\n## lm(formula = bodyweight_kg ~ season, data = season_wt)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -6.1395 -2.3874 -0.3115  2.2229  5.6238 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  100.33932    1.59114  63.061   <2e-16 ***\n## seasonspring   2.24135    2.25021   0.996   0.3340    \n## seasonsummer  -0.05219    2.25021  -0.023   0.9818    \n## seasonwinter   5.17389    2.25021   2.299   0.0353 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.558 on 16 degrees of freedom\n## Multiple R-squared:  0.3105, Adjusted R-squared:  0.1812 \n## F-statistic: 2.402 on 3 and 16 DF,  p-value: 0.1057"},{"path":"multiple-regression.html","id":"equivalence-between-multiple-regression-and-one-way-anova","chapter":"3 Multiple regression","heading":"3.3 Equivalence between multiple regression and one-way ANOVA","text":"wanted see whether bodyweight varies season, one way ANOVA season_wt2 like .OK, now can replicate result using regression model ?\\[Y_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{3i} + e_i\\]Note \\(F\\) values \\(p\\) values identical two methods!","code":"\n## make season into a factor with baseline level 'winter'\nseason_wt3 <- season_wt2 %>%\n  mutate(season = factor(season, levels = c(\"winter\", \"spring\",\n                                            \"summer\", \"fall\")))\n\nmy_anova <- aov(bodyweight_kg ~ season, season_wt3)\nsummary(my_anova)##             Df Sum Sq Mean Sq F value Pr(>F)\n## season       3  91.21   30.40   2.402  0.106\n## Residuals   16 202.54   12.66\nsummary(lm(bodyweight_kg ~ spring_v_winter +\n             summer_v_winter + fall_v_winter,\n           season_wt2))## \n## Call:\n## lm(formula = bodyweight_kg ~ spring_v_winter + summer_v_winter + \n##     fall_v_winter, data = season_wt2)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -6.1395 -2.3874 -0.3115  2.2229  5.6238 \n## \n## Coefficients:\n##                 Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)      105.513      1.591  66.313   <2e-16 ***\n## spring_v_winter   -2.933      2.250  -1.303   0.2109    \n## summer_v_winter   -5.226      2.250  -2.322   0.0337 *  \n## fall_v_winter     -5.174      2.250  -2.299   0.0353 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.558 on 16 degrees of freedom\n## Multiple R-squared:  0.3105, Adjusted R-squared:  0.1812 \n## F-statistic: 2.402 on 3 and 16 DF,  p-value: 0.1057"},{"path":"multiple-regression.html","id":"solutions-to-exercises","chapter":"3 Multiple regression","heading":"3.4 Solutions to exercises","text":"First create tibble new predictors. might also want know range values nclicks varies .Now plot.\nFigure 3.2: Partial effect plot nclicks grade.\n","code":"\nlecture_mean <- grades %>% pull(lecture) %>% mean()\nmin_nclicks <- grades %>% pull(nclicks) %>% min()\nmax_nclicks <- grades %>% pull(nclicks) %>% max()\n\n## new data for prediction\nnew_nclicks <- tibble(lecture = lecture_mean,\n                      nclicks = min_nclicks:max_nclicks)\n\n## add the predicted value to new_lecture\nnew_nclicks2 <- new_nclicks %>%\n  mutate(grade = predict(my_model, new_nclicks))\n\nnew_nclicks2## # A tibble: 76 × 3\n##    lecture nclicks grade\n##      <dbl>   <int> <dbl>\n##  1    6.99      54  2.37\n##  2    6.99      55  2.38\n##  3    6.99      56  2.38\n##  4    6.99      57  2.39\n##  5    6.99      58  2.39\n##  6    6.99      59  2.40\n##  7    6.99      60  2.40\n##  8    6.99      61  2.41\n##  9    6.99      62  2.41\n## 10    6.99      63  2.42\n## # ℹ 66 more rows\nggplot(grades, aes(nclicks, grade)) +\n  geom_point() +\n  geom_line(data = new_nclicks2)"},{"path":"interactions.html","id":"interactions","chapter":"4 Interactions","heading":"4 Interactions","text":"now, focusing estimating interpreting effect variable linear combination predictor variables response variable. However, often situations effect one predictor response depends value another predictor variable. can actually estimate interpret dependency well, including interaction term model.","code":""},{"path":"interactions.html","id":"cont-by-cat","chapter":"4 Interactions","heading":"4.1 Continuous-by-Categorical Interactions","text":"One common example interested whether linear relationship continous predictor continuous response different two groups.consider simple fictional example. Say interested effects sonic distraction cognitive performance. participant study randomly assigned receive particular amount sonic distraction perform simple reaction time task (respond quickly possible flashing light). technique allows automatically generate different levels background noise (e.g., frequency amplitude city sounds, sirens, jackhammers, people yelling, glass breaking, etc.). participant performs task randomly chosen level distraction (0 100). hypothesis urban living makes people's task performance immune sonic distraction. want compare relationship distraction performance city dwellers relationship people quieter rural environments.three variables:continuous response variable, mean_RT, higher levels reflecting slower RTs;continuous predictor variable, level sonic distraction (dist_level), higher levels indicating distraction;factor two levels, group (urban vs. rural).start simulating data urban group. assume zero distraction (silence), average RT 450 milliseconds, unit increase distraction scale, RT increases 2 ms. gives us following linear model:\\[Y_i = 450 + 2 X_i + e_i\\]\\(X_i\\) amount sonic distraction.simulate data 100 participants \\(\\sigma = 80\\), setting seed begin.plot data created, along line best fit.\nFigure 4.1: Effect sonic distraction simple RT, urban group.\nNow simulate data rural group. assume participants perhaps little higher intercept, maybe less familiar technology. importantly, assume steeper slope affected noise. Something like:\\[Y_i = 500 + 3 X_i + e_i\\]Now plot data two groups side side.\nFigure 4.2: Effect sonic distraction simple RT urban rural participants.\nsee clearly difference slope built data. test whether two slopes significantly different? , two separate regressions. need bring two regression lines model. ?Note can represent one regression lines terms 'offset' values . (arbitrarily) choose one group 'baseline' group, represent y-intercept slope group offsets baseline. choose urban group baseline, can express y-intercept slope rural group terms two offsets, \\(\\beta_2\\) \\(\\beta_3\\), y-intercept slope, respectively.y-intercept: \\(\\beta_{0\\_rural} = \\beta_{0\\_urban} + \\beta_2\\)slope: \\(\\beta_{1\\_rural} = \\beta_{1\\_urban} + \\beta_3\\)urban group parameters \\(\\beta_{0\\_urban} = 450\\) \\(\\beta_{1\\_urban} = 2\\), whereas rural group \\(\\beta_{0\\_rural} = 500\\) \\(\\beta_{1\\_rural} = 3\\). directly follows :\\(\\beta_2 = 50\\), \\(\\beta_{0\\_rural} - \\beta_{0\\_urban} = 500 - 450 = 50\\), \\(\\beta_3 = 1\\), \\(\\beta_{1\\_rural} - \\beta_{1\\_urban} = 3 - 2 = 1\\).two regression models now:\\[Y_{\\_urban} = \\beta_{0\\_urban} + \\beta_{1\\_urban} X_i + e_i\\]\\[Y_{\\_rural} = (\\beta_{0\\_urban} + \\beta_2) + (\\beta_{1\\_urban} + \\beta_3) X_i + e_i.\\]OK, seems like closer getting single regression model. final trick. define additional dummy predictor variable takes value 0 urban group (chose 'baseline' group) 1 group. box contains final model.Regression model continuous--categorical interaction.\\[Y_{} = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{1i} X_{2i} + e_{}\\]\\(X_{1i}\\) continous predictor, \\(X_{2i}\\) dummy-coded variable taking 0 baseline, 1 alternative group.Interpretation parameters:\\(\\beta_0\\): y-intercept baseline group;\\(\\beta_1\\): slope baseline group;\\(\\beta_2\\): offset y-intercept alternative group;\\(\\beta_3\\): offset slope alternative group.Estimation R:lm(Y ~ X1 + X2 + X1:X2, data) , shortcut:lm(Y ~ X1 * X2) * means \"possible main effects interactions X1 X2\"term \\(\\beta_3 X_{1i} X_{2i}\\), two predictors multiplied together, called interaction term. now show GLM gives us two regression lines want.derive regression equation urban group, plug 0 \\(X_{2i}\\). gives us\\[Y_{} = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 0 + \\beta_3 X_{1i} 0 + e_i\\], dropping terms zero, just\\[Y_{} = \\beta_0 + \\beta_1 X_{1i} + e_i,\\]regression equation baseline (urban) group. Compare \\(Y_{\\_urban}\\) .Plugging 1 \\(X_{2i}\\) give us equation rural group. get\\[Y_{} = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 1 + \\beta_3 X_{1i} 1 + e_i\\], reducing applying little algebra, can also expressed \\[Y_{} = \\beta_0 + \\beta_2 + (\\beta_1 + \\beta_3) X_{1i} + e_i.\\]Compare \\(Y_{\\_rural}\\) . dummy-coding trick works!estimate regression coefficients R. say wanted test hypothesis slopes two lines different. Note just amounts testing null hypothesis \\(\\beta_3 = 0\\), \\(\\beta_3\\) slope offset. parameter zero, means single slope groups (although can different intercepts). words, means two slopes parallel. non-zero, means two groups different slopes; , two slopes parallel.Parallel lines sample versus populationI just said two non-parallel lines mean interaction categorical continuous predictors, parallel lines mean interaction. important clear talking whether lines parallel population. Whether parallel sample depends status population, also biases introduced measurement sampling. Lines parallel population nonetheless extremely likely give rise lines sample slopes appear different, especially sample small.Generally, interested whether slopes different population, sample. reason, just look graph sample data reason, \"lines parallel must interaction\", vice versa, \"lines look parallel, interaction.\" must run inferential statistical test.interaction term statistically significant \\(\\alpha\\) level (e.g., 0.05), reject null hypothesis interaction coefficient zero (e.g., \\(H_0: \\beta_3 = 0\\)), implies lines parallel population.However, non-significant interaction necessarily imply lines parallel population. might , also possible , study just lacked sufficient power detect difference.best can get evidence null hypothesis run called equivalence test, seek reject null hypothesis population effect larger smallest effect size interest; see Lakens et al. (2018) tutorial.already created dataset all_data combining simulated data two groups. way express model using R formula syntax Y ~ X1 + X2 + X1:X2 X1:X2 tells R create predictor product predictors X1 X2. shortcut Y ~ X1 * X2 tells R calculate possible main effects interactions. First add dummy predictor model, storing result all_data2.ExercisesFill blanks . Type answers least two decimal places.Given regression model\\[Y_{} = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{1i} X_{2i} + e_{}\\](\\(X_{1i}\\) continuous predictor \\(X_{2i}\\) categorical predictor) output lm() , identify following parameter estimates.\\(\\hat{\\beta}_0\\): \\(\\hat{\\beta}_1\\): \\(\\hat{\\beta}_2\\): \\(\\hat{\\beta}_3\\): Based parameter estimates, regression line (baseline) urban group :\\(Y_i =\\) \\(+\\) \\(X_{1i}\\)regression line rural group :\\(Y_i =\\) \\(+\\) \\(X_{1i}\\)\\(\\beta_0=\\) 460.11\\(\\beta_1=\\) 1.91\\(\\beta_2=\\) 4.83\\(\\beta_3=\\) 1.59The regression line urban group just\\(Y_i = \\beta_0 + \\beta_1 X_{1i}\\) \\(Y_i =\\) 460.11 \\(+\\) 1.91 \\(X_{1i}\\)line rural group \\(Y_i = \\beta_0 + \\beta_2 + \\left(\\beta_1 + \\beta_3\\right) X_{1i}\\) \\(Y_i=\\) 464.93 \\(+\\) 3.5 \\(X_{1i}\\)","code":"\nlibrary(\"tidyverse\")\nset.seed(1031)\n\nn_subj <- 100L  # simulate data for 100 subjects\nb0_urban <- 450 # y-intercept\nb1_urban <- 2   # slope\n\n# decomposition table\nurban <- tibble(\n  subj_id = 1:n_subj,\n  group = \"urban\",\n  b0 = 450,\n  b1 = 2,\n  dist_level = sample(0:n_subj, n_subj, replace = TRUE),\n  err = rnorm(n_subj, mean = 0, sd = 80),\n  simple_rt = b0 + b1 * dist_level + err)\n\nurban## # A tibble: 100 × 7\n##    subj_id group    b0    b1 dist_level     err simple_rt\n##      <int> <chr> <dbl> <dbl>      <int>   <dbl>     <dbl>\n##  1       1 urban   450     2         59  -36.1       532.\n##  2       2 urban   450     2         45  128.        668.\n##  3       3 urban   450     2         55   23.5       584.\n##  4       4 urban   450     2          8    1.04      467.\n##  5       5 urban   450     2         47   48.7       593.\n##  6       6 urban   450     2         96   88.2       730.\n##  7       7 urban   450     2         62  110.        684.\n##  8       8 urban   450     2          8  -91.6       374.\n##  9       9 urban   450     2         15 -109.        371.\n## 10      10 urban   450     2         70   20.7       611.\n## # ℹ 90 more rows\nggplot(urban, aes(dist_level, simple_rt)) + \n  geom_point(alpha = .2) +\n  geom_smooth(method = \"lm\", se = FALSE)## `geom_smooth()` using formula = 'y ~ x'\nb0_rural <- 500\nb1_rural <- 3\n\nrural <- tibble(\n  subj_id = 1:n_subj + n_subj,\n  group = \"rural\",\n  b0 = b0_rural,\n  b1 = b1_rural,\n  dist_level = sample(0:n_subj, n_subj, replace = TRUE),\n  err = rnorm(n_subj, mean = 0, sd = 80),\n  simple_rt = b0 + b1 * dist_level + err)\nall_data <- bind_rows(urban, rural)\n\nggplot(all_data %>% mutate(group = fct_relevel(group, \"urban\")), \n       aes(dist_level, simple_rt, colour = group)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  facet_wrap(~ group) + \n  theme(legend.position = \"none\")## `geom_smooth()` using formula = 'y ~ x'\nall_data2 <- all_data %>%\n  mutate(grp = if_else(group == \"rural\", 1, 0))\nsonic_mod <- lm(simple_rt ~ dist_level + grp + dist_level:grp,\n                all_data2)\n\nsummary(sonic_mod)## \n## Call:\n## lm(formula = simple_rt ~ dist_level + grp + dist_level:grp, data = all_data2)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -261.130  -50.749    3.617   62.304  191.211 \n## \n## Coefficients:\n##                Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)    460.1098    15.5053  29.674  < 2e-16 ***\n## dist_level       1.9123     0.2620   7.299 7.07e-12 ***\n## grp              4.8250    21.7184   0.222    0.824    \n## dist_level:grp   1.5865     0.3809   4.166 4.65e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 81.14 on 196 degrees of freedom\n## Multiple R-squared:  0.5625, Adjusted R-squared:  0.5558 \n## F-statistic: 83.99 on 3 and 196 DF,  p-value: < 2.2e-16"},{"path":"interactions.html","id":"categorical-by-categorical-interactions","chapter":"4 Interactions","heading":"4.2 Categorical-by-Categorical Interactions","text":"Factorial designs common psychology, often analyzed using ANOVA-based techniques, can obscure fact ANOVA, like regression, also assumes underlying linear model.factorial design one predictors (IVs) categorical: factor fixed number levels. full-factorial design, factors fully crossed possible combination factors represented. call unique combination cell design. often hear designs referred \"two--two design\" (2x2), means two factors, two levels. \"three--three\" (3x3) design one two factors, three levels; \"two--two--two\" 2x2x2 design one three factors, two levels; .Typically, factorial designs given tabular representation, showing combinations factor levels. tabular representation 2x2 design.3x2 design might shown follows.finally, 2x2x2 design.\\[C_1\\]\\[C_2\\]confuse factors levels!hear study three treatment groups (treatment , treatment B, control), \"three-factor (three-way) design\". one-factor (one-way) design single three-level factor (treatment condition).thing factor one level.can find many cells design multiplying number levels factor. , 2x3x4 design 24 cells design.","code":""},{"path":"interactions.html","id":"effects-of-cognitive-therapy-and-drug-therapy-on-mood","chapter":"4 Interactions","heading":"4.2.1 Effects of cognitive therapy and drug therapy on mood","text":"consider simple factorial design think types patterns data can show. get concepts concrete example, map onto abstract statistical terminology.Imagine running study looking effects two different types therapy depressed patients, cognitive therapy drug therapy. Half participants randomly assigned receive Cognitive Behavioral Therapy (CBT) half get kind control activity. Also, divide patients random assignment drug therapy group, whose members receive anti-depressants, control group, whose members receive placebo. treatment (control/placebo), measure mood scale, higher numbers corresponding positive mood.imagine means obtain population means, free measurement sampling error. take moment consider three different possible outcomes imply therapies might work independently interactively affect mood.reminder populations samples categorical--continuous interactions also applies . Except simulating data, almost never know true means population studying. , talking hypothetical situation actually know population means can draw conclusions without statistical tests. real sample means look include sampling measurement error, inferences make depend outcome statistical tests, rather observed pattern means.","code":""},{"path":"interactions.html","id":"scenario-a","chapter":"4 Interactions","heading":"Scenario A","text":"\nFigure 4.3: Scenario , plot cell means.\ntable cell means marginal means. cell means mean values dependent variable (mood) cell design. marginal means (margins table) provide means row column.\nTable 4.1: Table 4.2: Scenario , Table Means.\noutcome, conclude? cognitive therapy effect mood? drug therapy. answer questions yes: mean mood people got CBT (70; mean column 2) 20 points higher mean mood people (50; mean column 1).Likewise, people got anti-depressants showed enhanced mood (70; mean row 2) relative people got placebo (50; mean row 1).Now can also ask following question: effect cognitive therapy depend whether patient simultaneously receiving drug therapy? answer , . see , note Placebo group (Row 1), cognitive therapy increased mood 20 points (40 60). Drug group: increase 20 points 60 80. , evidence effect one factor mood depends .","code":""},{"path":"interactions.html","id":"scenario-b","chapter":"4 Interactions","heading":"Scenario B","text":"\nFigure 4.4: Scenario B, plot cell means.\n\nTable 4.3: Table 4.4: Scenario B, Table Means.\nscenario, also see CBT improved mood (, 20 points), effect Drug Therapy (equal marginal means 50 row 1 row 2). can also see effect CBT also depend upon Drug therapy; increase 20 points row.","code":""},{"path":"interactions.html","id":"scenario-c","chapter":"4 Interactions","heading":"Scenario C","text":"\nFigure 4.5: Scenario C, plot cell means.\n\nTable 4.5: Table 4.6: Scenario C, Table Means.\nFollowing logic previous sections, see overall, people got cognitive therapy showed elevated mood relative control (75 vs 45), people got drug therapy also showed elevated mood relative placebo (70 vs 50). something else going : seems effect cognitive therapy mood pronounced patients also receiving drug therapy. patients antidepressants, 40 point increase mood relative control (50 90; row 2 table). patients got placebo, 20 point increase mood, 40 60 (row 1 table). hypothetical scenario, effect cognitive therapy depends whether also ongoing drug therapy.","code":""},{"path":"interactions.html","id":"effects-in-a-factorial-design","chapter":"4 Interactions","heading":"4.2.2 Effects in a factorial design","text":"understand basic patterns effects described previous section, ready map concepts onto statistical language.","code":""},{"path":"interactions.html","id":"main-effect","chapter":"4 Interactions","heading":"4.2.2.1 Main effect","text":"Main effect: effect factor DV ignoring factors design.test main effect test equivalence marginal means. Scenario , compared row means drug therapy, assessing main effect factor mood. null hypothesis two marginal means equal:\\[\\bar{Y}_{1..} = \\bar{Y}_{2..}\\]\\(Y_{..}\\) mean row \\(\\), ignoring column factor.factor \\(k\\) levels \\(k > 2\\), null hypothesis main effect \\[\\bar{Y}_{1..} = \\bar{Y}_{2..} = \\ldots = \\bar{Y}_{k..},\\].e., row (column) means equal.","code":""},{"path":"interactions.html","id":"simple-effect","chapter":"4 Interactions","heading":"4.2.2.2 Simple effect","text":"Simple effect effect one factor specific level another factor (.e., holding factor constant particular value).instance, Scenario C, talked effect CBT participants anti-depressant group. case, simple effect CBT participants receiving anti-depressants 40 units.also talk simple effect drug therapy patients received cognitive therapy. Scenario C, increase mood 60 90 (column 2).","code":""},{"path":"interactions.html","id":"interaction","chapter":"4 Interactions","heading":"4.2.2.3 Interaction","text":"say interaction present effect one variable differs across levels another variable.mathematical definition interaction present simple effects one factor differ across levels another factor. saw Scenario C, 40 point boost CBT anti-depressant group, compared boost 20 placebo group. Perhaps elevated mood caused anti-depressants made patients susceptable CBT.main point say simple interaction B simple effects differ across levels B. also check whether simple effects B differ across . possible one statements true without also true, matter way look simple effects.","code":""},{"path":"interactions.html","id":"higher-order-designs","chapter":"4 Interactions","heading":"4.2.3 Higher-order designs","text":"Two-factor (also known \"two-way\") designs common psychology neuroscience, sometimes also see designs two factors, 2x2x2 design.figure number effects different kinds, use formula , gives us number possible combinations \\(n\\) elements take \\(k\\) time:\\[\\frac{n!}{k!(n - k)!}\\]Rather actually computing hand, can just use choose(n, k) function R.design \\(n\\) factors, :\\(n\\) main effects;\\(\\frac{n!}{2!(n - 2)!}\\) two-way interactions;\\(\\frac{n!}{3!(n - 3)!}\\) three-way interactions;\\(\\frac{n!}{4!(n - 4)!}\\) four-way interactions... forth.three-way design, e.g., 2x2x2 factors \\(\\), \\(B\\), \\(C\\), 3 main effects: \\(\\), \\(B\\), \\(C\\). choose(3, 2) = three two way interactions: \\(AB\\), \\(AC\\), \\(BC\\), choose(3, 3) = one three-way interaction: \\(ABC\\).Three-way interactions hard interpret, imply simple interaction two given factors varies across level third factor. example, imply \\(AB\\) interaction \\(C_1\\) different \\(AB\\) interaction \\(C_2\\).four way design, four main effects, choose(4, 2) =6 two-way interactions, choose(4, 3) =4 three-way interactions, one four-way interaction. next impossible interpret results four-way design, keep designs simple!","code":""},{"path":"interactions.html","id":"the-glm-for-a-factorial-design","chapter":"4 Interactions","heading":"4.3 The GLM for a factorial design","text":"Now look math behind models. typically way see GLM ANOVA written 2x2 factorial design uses \"ANOVA\" notation, like :\\[Y_{ijk} = \\mu + A_i + B_j + AB_{ij} + S(AB)_{ijk}.\\]formula,\\(Y_{ijk}\\) score observation \\(k\\) level \\(\\) \\(\\) level \\(j\\) \\(B\\);\\(\\mu\\) grand mean;\\(A_i\\) main effect factor \\(\\) level \\(\\) \\(\\);\\(B_j\\) main effect factor \\(B\\) level \\(j\\) \\(B\\);\\(AB_{ij}\\) \\(AB\\) interaction level \\(\\) \\(\\) level \\(j\\) \\(B\\);\\(S(AB)_{ijk}\\) residual.important mathematical fact individual main interaction effects sum zero, often written :\\(\\Sigma_i A_i = 0\\);\\(\\Sigma_j B_j = 0\\);\\(\\Sigma_{ij} AB_{ij} = 0\\).best way understand effects see decomposition table. Study decomposition table belo wfor 12 simulated observations 2x2 design factors \\(\\) \\(B\\). indexes \\(\\), \\(j\\), \\(k\\) provided just help keep track observation dealing . Remember \\(\\) indexes level factor \\(\\), \\(j\\) indexes level factor \\(B\\), \\(k\\) indexes observation number within cell \\(AB_{ij}\\).","code":"## # A tibble: 12 × 9\n##        Y     i     j     k    mu   A_i   B_j AB_ij   err\n##    <dbl> <int> <int> <int> <dbl> <dbl> <dbl> <dbl> <int>\n##  1    11     1     1     1    10     4    -2    -1     0\n##  2    14     1     1     2    10     4    -2    -1     3\n##  3     8     1     1     3    10     4    -2    -1    -3\n##  4    17     1     2     1    10     4     2     1     0\n##  5    15     1     2     2    10     4     2     1    -2\n##  6    19     1     2     3    10     4     2     1     2\n##  7     8     2     1     1    10    -4    -2     1     3\n##  8     4     2     1     2    10    -4    -2     1    -1\n##  9     3     2     1     3    10    -4    -2     1    -2\n## 10    10     2     2     1    10    -4     2    -1     3\n## 11     7     2     2     2    10    -4     2    -1     0\n## 12     4     2     2     3    10    -4     2    -1    -3"},{"path":"interactions.html","id":"estimation-equations","chapter":"4 Interactions","heading":"4.3.1 Estimation equations","text":"equations used estimate effects ANOVA.\\(\\hat{\\mu} = Y_{...}\\)\\(\\hat{}_i = Y_{..} - \\hat{\\mu}\\)\\(\\hat{B}_j = Y_{.j.} - \\hat{\\mu}\\)\\(\\widehat{AB}_{ij} = Y_{ij.} - \\hat{\\mu} - \\hat{}_i - \\hat{B}_j\\)Note \\(Y\\) variable dots subscripts means \\(Y\\), taken ignoring anything appearing dot. \\(Y_{...}\\) mean \\(Y\\), \\(Y_{..}\\) mean \\(Y\\) level \\(\\) \\(\\), \\(Y_{.j.}\\) mean \\(Y\\) level \\(j\\) \\(B\\), \\(Y_{ij.}\\) mean \\(Y\\) level \\(\\) \\(\\) level \\(j\\) \\(B\\), .e., cell mean \\(ij\\).","code":""},{"path":"interactions.html","id":"factorial-app","chapter":"4 Interactions","heading":"4.3.2 Factorial App","text":"Launch web application experiment factorial designs understand key concepts main effects interactions factorial design.","code":""},{"path":"interactions.html","id":"code-your-own-categorical-predictors-in-factorial-designs","chapter":"4 Interactions","heading":"4.4 Code your own categorical predictors in factorial designs","text":"Many studies psychology---especially experimental psychology---involve categorical independent variables. Analyzing data studies requires care specifying predictors, defaults R ideal experimental situations. main problem default coding categorical predictors gives simple effects rather main effects output, usually want latter. People sometimes unaware misinterpret output. also happens researchers report results regression categorical predictors explicitly report coded , making findings potentially difficult interpret irreproducible. interest reproducibility, transparency, accurate interpretation, good idea learn code categorical predictors \"hand\" get habit reporting reports.R defaults good factorial designs, going suggest always code categorical variables including predictors linear models. include factor variables.","code":""},{"path":"interactions.html","id":"coding-schemes-for-categorical-variables","chapter":"4 Interactions","heading":"4.5 Coding schemes for categorical variables","text":"Many experimentalists trying make leap ANOVA linear mixed-effects models (LMEMs) R struggle coding categorical predictors. unexpectedly complicated, defaults provided R turn wholly inappropriate factorial experiments. Indeed, using defaults factorial experiments can lead researchers draw erroneous conclusions data.keep things simple, start situations design factors two levels moving designs three levels.","code":""},{"path":"interactions.html","id":"simple-versus-main-effects","chapter":"4 Interactions","heading":"4.5.1 Simple versus main effects","text":"important understand difference simple effect main effect, simple interaction main interaction three-way design.\\({\\times}B\\) design, simple effect \\(\\) effect \\(\\) controlling \\(B\\), main effect \\(\\) effect \\(\\) ignoring \\(B\\). Another way looking consider cell means (\\(\\bar{Y}_{11}\\), \\(\\bar{Y}_{12}\\), \\(\\bar{Y}_{21}\\), \\(\\bar{Y}_{22}\\)) marginal means (\\(\\bar{Y}_{1.}\\), \\(\\bar{Y}_{2.}\\), \\(\\bar{Y}_{.1}\\), \\(\\bar{Y}_{.2}\\)) factorial design. (dot subscript tells \"ignore\" dimension containing dot; e.g., \\(\\bar{Y}_{.1}\\) tells take mean first column ignoring row variable.) test main effect test null hypothesis \\(\\bar{Y}_{1.}=\\bar{Y}_{2.}\\). test simple effect \\(\\)—effect \\(\\) particular level \\(B\\)—, instance, test null hypothesis \\(\\bar{Y}_{11}=\\bar{Y}_{21}\\).distinction simple interactions main interactions logic: simple interaction \\(AB\\) \\(ABC\\) design interaction \\(AB\\) particular level \\(C\\); main interaction \\(AB\\) interaction ignoring C. latter usually talking talk lower-order interactions three-way design. also given output standard ANOVA procedures, e.g., aov() function R, SPSS, SAS, etc.","code":""},{"path":"interactions.html","id":"the-key-coding-schemes","chapter":"4 Interactions","heading":"4.5.2 The key coding schemes","text":"Generally, choice coding scheme impacts interpretation :intercept term; andthe interpretation tests highest-order effects interactions factorial design.also can influence interpretation/estimation random effects mixed-effects model (see blog post discussion). design single two-level factor, using maximal random-effects structure, choice coding scheme really matter.many possible coding schemes (see ?contr.treatment information). relevant ones treatment, sum, deviation. Sum deviation coding can seen special cases effect coding; effect coding, people generally mean codes sum zero.two-level factor, use following codes:default R use treatment coding variable defined =factor= model (see ?factor ?contrasts information). see ideal factorial designs, consider 2x2x2 factorial design factors \\(\\), \\(B\\) \\(C\\). just consider fully -subjects design one observation per subject allows us use simplest possible error structure. fit model using lm():lm(Y ~ * B * C)\nlm(Y ~ * B * C)figure spells notation various cell marginal means 2x2x2 design.\\[C_1\\]\\[C_2\\]table provides interpretation various effects model three different coding schemes. Note \\(Y\\) dependent variable, dots subscript mean \"ignore\" corresponding dimension. Thus, \\(\\bar{Y}_{.1.}\\) mean B_1 (ignoring factors \\(\\) \\(C\\)) \\(\\bar{Y}_{...}\\) \"grand mean\" (ignoring factors).three way \\(\\times B \\times C\\) interaction:Note inferential tests \\(\\times B \\times C\\) outcome, despite parameter estimate sum coding one-eighth schemes. lower-order effects, sum deviation coding give different parameter estimates identical inferential outcomes. schemes provide identical tests canonical main effects main interactions three-way ANOVA. contrast, treatment (dummy) coding provide inferential tests simple effects simple interactions. , interested getting \"canonical\" tests ANOVA, use sum deviation coding.","code":""},{"path":"interactions.html","id":"what-about-factors-with-more-than-two-levels","chapter":"4 Interactions","heading":"4.5.3 What about factors with more than two levels?","text":"factor \\(k\\) levels requires \\(k-1\\) variables. predictor contrasts particular \"target\" level factor level (arbitrarily) choose \"baseline\" level. instance, three-level factor \\(\\) \\(A1\\) chosen baseline, two predictor variables, one compares \\(A2\\) \\(A1\\) compares \\(A3\\) \\(A1\\).treatment (dummy) coding, target level set 1, otherwise 0.sum coding, levels must sum zero, given predictor, target level given value 1, baseline level given value -1, level given value 0.deviation coding, values must also sum 0. Deviation coding recommended whenever trying draw ANOVA-style inferences. scheme, target level gets value \\(\\frac{k-1}{k}\\) non-target level gets value \\(-\\frac{1}{k}\\).Fun fact: Mean-centering treatment codes (balanced data) give deviation codes.","code":""},{"path":"interactions.html","id":"example-three-level-factor","chapter":"4 Interactions","heading":"4.5.4 Example: Three-level factor","text":"","code":""},{"path":"interactions.html","id":"treatment-dummy","chapter":"4 Interactions","heading":"4.5.4.1 Treatment (Dummy)","text":"","code":""},{"path":"interactions.html","id":"sum","chapter":"4 Interactions","heading":"4.5.4.2 Sum","text":"","code":""},{"path":"interactions.html","id":"deviation","chapter":"4 Interactions","heading":"4.5.4.3 Deviation","text":"","code":""},{"path":"interactions.html","id":"example-five-level-factor","chapter":"4 Interactions","heading":"4.5.4.4 Example: Five-level factor","text":"","code":""},{"path":"interactions.html","id":"treatment-dummy-1","chapter":"4 Interactions","heading":"4.5.4.5 Treatment (Dummy)","text":"","code":""},{"path":"interactions.html","id":"sum-1","chapter":"4 Interactions","heading":"4.5.4.6 Sum","text":"","code":""},{"path":"interactions.html","id":"deviation-1","chapter":"4 Interactions","heading":"4.5.4.7 Deviation","text":"","code":""},{"path":"interactions.html","id":"how-to-create-your-own-numeric-predictors","chapter":"4 Interactions","heading":"4.5.5 How to create your own numeric predictors","text":"assume data contained table dat like one .","code":"\n ## create your own numeric predictors\n ## make an example table\n dat <- tibble(Y = rnorm(12),\n               A = rep(paste0(\"A\", 1:3), each = 4))"},{"path":"interactions.html","id":"the-mutate-if_else-case_when-approach-for-a-three-level-factor","chapter":"4 Interactions","heading":"4.5.5.1 The mutate() / if_else() / case_when() approach for a three-level factor","text":"","code":""},{"path":"interactions.html","id":"treatment","chapter":"4 Interactions","heading":"4.5.5.2 Treatment","text":"","code":"\n  ## examples of three level factors\n  ## treatment coding\n  dat_treat <- dat %>%\n    mutate(A2v1 = if_else(A == \"A2\", 1L, 0L),\n       A3v1 = if_else(A == \"A3\", 1L, 0L))## # A tibble: 12 × 4\n##         Y A      A2v1  A3v1\n##     <dbl> <chr> <int> <int>\n##  1 -0.410 A1        0     0\n##  2 -1.44  A1        0     0\n##  3 -2.01  A1        0     0\n##  4  0.562 A1        0     0\n##  5 -0.671 A2        1     0\n##  6  1.00  A2        1     0\n##  7 -1.61  A2        1     0\n##  8  0.322 A2        1     0\n##  9 -0.443 A3        0     1\n## 10  1.19  A3        0     1\n## 11  0.868 A3        0     1\n## 12 -0.500 A3        0     1"},{"path":"interactions.html","id":"sum-2","chapter":"4 Interactions","heading":"4.5.5.3 Sum","text":"","code":"\n## sum coding\ndat_sum <- dat %>%\n  mutate(A2v1 = case_when(A == \"A1\" ~ -1L, # baseline\n                          A == \"A2\" ~ 1L,  # target\n                          TRUE      ~ 0L), # anything else\n         A3v1 = case_when(A == \"A1\" ~ -1L, # baseline\n                          A == \"A3\" ~  1L, # target\n                          TRUE      ~ 0L)) # anything else## # A tibble: 12 × 4\n##         Y A      A2v1  A3v1\n##     <dbl> <chr> <int> <int>\n##  1 -0.410 A1       -1    -1\n##  2 -1.44  A1       -1    -1\n##  3 -2.01  A1       -1    -1\n##  4  0.562 A1       -1    -1\n##  5 -0.671 A2        1     0\n##  6  1.00  A2        1     0\n##  7 -1.61  A2        1     0\n##  8  0.322 A2        1     0\n##  9 -0.443 A3        0     1\n## 10  1.19  A3        0     1\n## 11  0.868 A3        0     1\n## 12 -0.500 A3        0     1"},{"path":"interactions.html","id":"deviation-2","chapter":"4 Interactions","heading":"4.5.5.4 Deviation","text":"","code":"\n## deviation coding\n## baseline A1\ndat_dev <- dat %>%\n  mutate(A2v1 = if_else(A == \"A2\", 2/3, -1/3), # target A2\n         A3v1 = if_else(A == \"A3\", 2/3, -1/3)) # target A3\ndat_dev## # A tibble: 12 × 4\n##         Y A       A2v1   A3v1\n##     <dbl> <chr>  <dbl>  <dbl>\n##  1 -0.410 A1    -0.333 -0.333\n##  2 -1.44  A1    -0.333 -0.333\n##  3 -2.01  A1    -0.333 -0.333\n##  4  0.562 A1    -0.333 -0.333\n##  5 -0.671 A2     0.667 -0.333\n##  6  1.00  A2     0.667 -0.333\n##  7 -1.61  A2     0.667 -0.333\n##  8  0.322 A2     0.667 -0.333\n##  9 -0.443 A3    -0.333  0.667\n## 10  1.19  A3    -0.333  0.667\n## 11  0.868 A3    -0.333  0.667\n## 12 -0.500 A3    -0.333  0.667"},{"path":"interactions.html","id":"conclusion","chapter":"4 Interactions","heading":"4.5.6 Conclusion","text":"interpretation highest order effect depends coding scheme.treatment coding, looking simple effects simple interactions, main effects main interactions.parameter estimates sum coding differs deviation coding magnitude parameter estimates, identical interpretations.subject scaling effects seen sum coding, deviation used default ANOVA-style designs.default coding scheme factors R \"treatment\" coding., anytime declare variable type factor use variable predictor regression model, R automatically create treatment-coded variables.Take-home message: analyzing factorial designs R using regression, obtain canonical ANOVA-style interpretations main effects interactions use deviation coding default treatment coding.","code":""},{"path":"introducing-linear-mixed-effects-models.html","id":"introducing-linear-mixed-effects-models","chapter":"5 Introducing linear mixed-effects models","heading":"5 Introducing linear mixed-effects models","text":"","code":""},{"path":"introducing-linear-mixed-effects-models.html","id":"modeling-multi-level-data","chapter":"5 Introducing linear mixed-effects models","heading":"5.1 Modeling multi-level data","text":"ideas chapter come textbook Statistical Rethinking McElreath (2020), chapter also borrows extensively Tristan Mahr's excellent blog post partial pooling.chapter, working real data study looking effects sleep deprivation psychomotor performance (Belenky et al., 2003). Data study included built-dataset sleepstudy lme4 package R (Bates et al., 2015).start looking documentation sleepstudy dataset. loading lme4 package, can access documentation typing ?sleepstudy console.data meet definition multilevel data due repeated measurements dependent variable (mean RT) participants ten days. Multilevel data type extremely common psychology. Unfortunately, statistics textbooks commonly used psychology courses sufficiently discuss multilevel data, beyond paired t-tests repeated-measures ANOVA. sleepstudy dataset interesting multilevel continuous predictor, thus fit t-test ANOVA, approaches categorical predictors. ways make data fit one frameworks, without losing information possibly violating assumptions.shame psychology students really learn much analyzing multilevel data. Think studies read recently psychology neuroscience. many take single measurement DV participant? , . Nearly take multiple measurements, one following reasons: (1) researchers measuring participants across levels factor within-subject design; (2) interested assessing change time; (3) measuring responses multiple stimuli. Multilevel data common multilevel analysis taught default approach psychology. Learning multilevel analysis can challenging, already know much need learned correlation regression. see just extension simple regression.take closer look sleepstudy data. dataset contains eighteen participants three-hour sleep condition. day, 10 days, participants performed ten-minute \"psychomotor vigilance test\" monitor display press button quickly possible time stimulus appeared. dependent measure dataset participant's average response time (RT) task day.good way start every analysis plot data. data single subject.\nFigure 5.1: Data single subject Belenky et al. (2003)\nExerciseUse ggplot recreate plot , shows data 18 subjects.\nFigure 5.2: Data Belenky et al. (2003)\nlooks like RT increasing additional day sleep deprivation, starting day 2 increasing day 10.Just , given code make plot single participant. Adapt code show participants getting rid filter() statement adding ggplot2 function starts facet_., except just add one line: facet_wrap(~Subject)","code":"sleepstudy                package:lme4                 R Documentation\n\nReaction times in a sleep deprivation study\n\nDescription:\n\n     The average reaction time per day (in milliseconds) for subjects\n     in a sleep deprivation study.\n\n     Days 0-1 were adaptation and training (T1/T2), day 2 was baseline\n     (B); sleep deprivation started after day 2.\n\nFormat:\n\n     A data frame with 180 observations on the following 3 variables.\n\n     ‘Reaction’ Average reaction time (ms)\n\n     ‘Days’ Number of days of sleep deprivation\n\n     ‘Subject’ Subject number on which the observation was made.\n\nDetails:\n\n     These data are from the study described in Belenky et al.  (2003),\n     for the most sleep-deprived group (3 hours time-in-bed) and for\n     the first 10 days of the study, up to the recovery period.  The\n     original study analyzed speed (1/(reaction time)) and treated day\n     as a categorical rather than a continuous predictor.\n\nReferences:\n\n     Gregory Belenky, Nancy J. Wesensten, David R. Thorne, Maria L.\n     Thomas, Helen C. Sing, Daniel P. Redmond, Michael B. Russo and\n     Thomas J. Balkin (2003) Patterns of performance degradation and\n     restoration during sleep restriction and subsequent recovery: a\n     sleep dose-response study. _Journal of Sleep Research_ *12*, 1-12.\nlibrary(\"lme4\")\nlibrary(\"tidyverse\")\n\njust_308 <- sleepstudy %>%\n  filter(Subject == \"308\")\n\nggplot(just_308, aes(x = Days, y = Reaction)) +\n  geom_point() +\n  scale_x_continuous(breaks = 0:9)\nggplot(sleepstudy, aes(x = Days, y = Reaction)) +\n  geom_point() +\n  scale_x_continuous(breaks = 0:9) +\n  facet_wrap(~Subject)"},{"path":"introducing-linear-mixed-effects-models.html","id":"how-to-model-these-data","chapter":"5 Introducing linear mixed-effects models","heading":"5.2 How to model these data?","text":"model data appropriately, first need know design. Belenky et al. (2003) describe study (p. 2):first 3 days (T1, T2 B) adaptation training (T1 T2) baseline (B) subjects required bed 23:00 07:00 h [8 h required time bed (TIB)]. third day (B), baseline measures taken. Beginning fourth day continuing total 7 days (E1–E7) subjects one four sleep conditions [9 h required TIB (22:00–07:00 h), 7 h required TIB (24:00–07:00 h), 5 h required TIB (02:00–07:00 h), 3 h required TIB (04:00–07:00 h)], effectively one sleep augmentation condition, three sleep restriction conditions.seven nights sleep restriction, first night restriction occurring third day. first two days, coded 0, 1, adaptation training. day coded 2, baseline measurement taken, place start analysis. include days 0 1 analysis, might bias results, since changes performance first two days training, sleep restriction.ExerciseRemove dataset observations Days coded 0 1, make new variable days_deprived Days variable sequence starts day 2, day 2 re-coded day 0, day 3 day 1, day 4 day 2, etc. new variable now tracks number days sleep deprivation. Store new table sleep2.always good idea double check code works intended. First, look :check Days days_deprived match .Looks good. Note variable n generated count() tells many rows unique combination Days days_deprived. case, 18, one row participant.Now re-plot data looking just eight data points Day 0 Day 7. just copied code , substituting sleep2 sleepstudy using days_deprived x variable.\nFigure 5.3: Data Belenky et al. (2003), showing reaction time baseline (0) day sleep deprivation.\nTake moment think might model relationship days_deprived Reaction. reaction time increase decrease increasing sleep deprivation? relationship roughly stable change time?one exception (subject 335) looks like reaction time increases additional day sleep deprivation. looks like fit line participant's data. Recall general equation line form y = y-intercept + slope \\(\\times\\) x. regression, usually express linear relationship formula\\[Y = \\beta_0 + \\beta_1 X\\]\\(\\beta_0\\) y-intercept \\(\\beta_1\\) slope, parameters whose values estimate data.lines differ intercept (mean RT day zero, sleep deprivation began) slope (change RT additional day sleep deprivation). fit line everyone? totally different line subject? something somewhere ?start considering three different approaches might take. Following McElreath, distinguish approaches calling complete pooling, pooling, partial pooling.","code":"\nsleep2 <- sleepstudy %>%\n  filter(Days >= 2L) %>%\n  mutate(days_deprived = Days - 2L)\nhead(sleep2)##   Reaction Days Subject days_deprived\n## 1 250.8006    2     308             0\n## 2 321.4398    3     308             1\n## 3 356.8519    4     308             2\n## 4 414.6901    5     308             3\n## 5 382.2038    6     308             4\n## 6 290.1486    7     308             5\nsleep2 %>%\n  count(days_deprived, Days)##   days_deprived Days  n\n## 1             0    2 18\n## 2             1    3 18\n## 3             2    4 18\n## 4             3    5 18\n## 5             4    6 18\n## 6             5    7 18\n## 7             6    8 18\n## 8             7    9 18\nggplot(sleep2, aes(x = days_deprived, y = Reaction)) +\n  geom_point() +\n  scale_x_continuous(breaks = 0:7) +\n  facet_wrap(~Subject) +\n  labs(y = \"Reaction Time\", x = \"Days deprived of sleep (0 = baseline)\")"},{"path":"introducing-linear-mixed-effects-models.html","id":"complete-pooling-one-size-fits-all","chapter":"5 Introducing linear mixed-effects models","heading":"5.2.1 Complete pooling: One size fits all","text":"complete pooling approach \"one-size-fits-\" model: estimates single intercept slope entire dataset, ignoring fact different subjects might vary intercepts slopes. sounds like bad approach, ; know already visualized data noted pattern participant seem require different y-intercept slope values.Fitting one line called \"complete pooling\" approach pool together data subjects get single estimates overall intercept slope. GLM approach simply\\[Y_{sd} = \\beta_0 + \\beta_1 X_{sd} + e_{sd}\\]\\[e_{sd} \\sim N\\left(0, \\sigma^2\\right)\\]\\(Y_{sd}\\) mean RT subject \\(s\\) day \\(d\\), \\(X_{sd}\\) value days_deprived associated case (0-7), \\(e_{sd}\\) error.fit model R using lm() function, e.g.:According model, predicted mean response time Day 0 268 milliseconds, increase 11 milliseconds per day deprivation, average. trust standard errors regression coefficients, however, assuming observations independent (technically, residuals ). However, can pretty sure bad assumption.add model predictions graph created . can use geom_abline() , specifying intercept slope line using regression coefficients model fit, coef(cp_model), returns two-element vector intercept slope, respectively.\nFigure 5.4: Data plotted predictions complete pooling model.\nmodel fits data badly. need different approach.","code":"\ncp_model <- lm(Reaction ~ days_deprived, sleep2)\n\nsummary(cp_model)## \n## Call:\n## lm(formula = Reaction ~ days_deprived, data = sleep2)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -112.284  -26.732    2.143   27.734  140.453 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)    267.967      7.737  34.633  < 2e-16 ***\n## days_deprived   11.435      1.850   6.183 6.32e-09 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 50.85 on 142 degrees of freedom\n## Multiple R-squared:  0.2121, Adjusted R-squared:  0.2066 \n## F-statistic: 38.23 on 1 and 142 DF,  p-value: 6.316e-09\ncoef(cp_model)##   (Intercept) days_deprived \n##     267.96742      11.43543\nggplot(sleep2, aes(x = days_deprived, y = Reaction)) +\n  geom_abline(intercept = coef(cp_model)[1],\n              slope = coef(cp_model)[2],\n              color = 'blue') +\n  geom_point() +\n  scale_x_continuous(breaks = 0:7) +\n  facet_wrap(~Subject) +\n  labs(y = \"Reaction Time\", x = \"Days deprived of sleep (0 = baseline)\")"},{"path":"introducing-linear-mixed-effects-models.html","id":"no-pooling","chapter":"5 Introducing linear mixed-effects models","heading":"5.2.2 No pooling","text":"Pooling information get just one intercept one slope estimate inappropriate. Another approach fit separate lines participant. means estimates participant completely uninformed estimates participants. words, can separately estimate 18 individual intercept/slope pairs.model implemented two ways: (1) running separate regressions participant (2) running fixed-effects regression. latter, everything one big model. know already: add dummy codes Subject factor. 18 levels factor, need 17 dummy codes. Fortunately, R saves us trouble creating 17 variables need hand. need include Subject predictor model, interact categorical predictor days_deprived allow intercepts slopes vary.variable Subject sleep2 dataset nominal. just use numbers labels preserve anonymity, without intending imply Subject 310 one point better Subject 309 two points better 308. Make sure define factor included continuous variable!can test whether something factor various ways. One use summary() table.can see treated number rather giving distributional information (means, etc.) tells many observations level.can also test directly:something factor, can make one re-defining using factor() function.model done take one subject baseline (specifically, subject 308), represent subject terms offsets baseline. saw already talked continuous--categorical interactions.Answer questions (three decimal places):intercept subject 308? slope subject 308? intercept subject 335? slope subject 335? baseline subject 308; default R sort levels factor alphabetically chooses first one baseline. means intercept slope 308 given (Intercept) days_deprived respectively, 17 dummy variables zero subject 308.regression coefficients subjects represented offsets baseline subject. want calculate intercept slope given subject, just add corresponding offsets. , answers areintercept 308: 288.217intercept 308: 288.217slope 308: 21.69slope 308: 21.69intercept 335: (Intercept) + Subject335 = 288.217 + -25.343 = 262.874intercept 335: (Intercept) + Subject335 = 288.217 + -25.343 = 262.874slope 335: days_deprived + days_deprived:Subject335 = 21.69 + -25.899 = -4.209slope 335: days_deprived + days_deprived:Subject335 = 21.69 + -25.899 = -4.209In \"pooling\" model, overall population intercept slope estimated; case, (Intercept) days_deprived estimates intercept slope subject 308, (arbitrarily) chosen baseline subject. get population estimates, introduce second stage analysis calculate means individual intercepts slopes. use model estimates calculate intercepts slopes subject.see well model fits data.\nFigure 5.5: Data plotted fits -pooling approach.\nmuch better complete pooling model. want test null hypothesis fixed slope zero, using one-sample test.tells us mean slope \n11.435\nsignificantly different zero,\nt(17) = 6.197, \\(p < .001\\).","code":"\nsleep2 %>% summary()##     Reaction          Days         Subject   days_deprived \n##  Min.   :203.0   Min.   :2.00   308    : 8   Min.   :0.00  \n##  1st Qu.:265.2   1st Qu.:3.75   309    : 8   1st Qu.:1.75  \n##  Median :303.2   Median :5.50   310    : 8   Median :3.50  \n##  Mean   :308.0   Mean   :5.50   330    : 8   Mean   :3.50  \n##  3rd Qu.:347.7   3rd Qu.:7.25   331    : 8   3rd Qu.:5.25  \n##  Max.   :466.4   Max.   :9.00   332    : 8   Max.   :7.00  \n##                                 (Other):96\nsleep2 %>% pull(Subject) %>% is.factor()## [1] TRUE\nnp_model <- lm(Reaction ~ days_deprived + Subject + days_deprived:Subject,\n               data = sleep2)\n\nsummary(np_model)## \n## Call:\n## lm(formula = Reaction ~ days_deprived + Subject + days_deprived:Subject, \n##     data = sleep2)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -106.521   -8.541    1.143    8.889  128.545 \n## \n## Coefficients:\n##                          Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)              288.2175    16.4772  17.492  < 2e-16 ***\n## days_deprived             21.6905     3.9388   5.507 2.49e-07 ***\n## Subject309               -87.9262    23.3023  -3.773 0.000264 ***\n## Subject310               -62.2856    23.3023  -2.673 0.008685 ** \n## Subject330               -14.9533    23.3023  -0.642 0.522422    \n## Subject331                 9.9658    23.3023   0.428 0.669740    \n## Subject332                27.8157    23.3023   1.194 0.235215    \n## Subject333                -2.7581    23.3023  -0.118 0.906000    \n## Subject334               -50.2051    23.3023  -2.155 0.033422 *  \n## Subject335               -25.3429    23.3023  -1.088 0.279207    \n## Subject337                24.6143    23.3023   1.056 0.293187    \n## Subject349               -59.2183    23.3023  -2.541 0.012464 *  \n## Subject350               -40.2023    23.3023  -1.725 0.087343 .  \n## Subject351               -24.2467    23.3023  -1.041 0.300419    \n## Subject352                43.0655    23.3023   1.848 0.067321 .  \n## Subject369               -21.5040    23.3023  -0.923 0.358154    \n## Subject370               -53.3072    23.3023  -2.288 0.024107 *  \n## Subject371               -30.4896    23.3023  -1.308 0.193504    \n## Subject372                 2.4772    23.3023   0.106 0.915535    \n## days_deprived:Subject309 -17.3334     5.5703  -3.112 0.002380 ** \n## days_deprived:Subject310 -17.7915     5.5703  -3.194 0.001839 ** \n## days_deprived:Subject330 -13.6849     5.5703  -2.457 0.015613 *  \n## days_deprived:Subject331 -16.8231     5.5703  -3.020 0.003154 ** \n## days_deprived:Subject332 -19.2947     5.5703  -3.464 0.000765 ***\n## days_deprived:Subject333 -10.8151     5.5703  -1.942 0.054796 .  \n## days_deprived:Subject334  -3.5745     5.5703  -0.642 0.522423    \n## days_deprived:Subject335 -25.8995     5.5703  -4.650 9.47e-06 ***\n## days_deprived:Subject337   0.7518     5.5703   0.135 0.892895    \n## days_deprived:Subject349  -5.2644     5.5703  -0.945 0.346731    \n## days_deprived:Subject350   1.6007     5.5703   0.287 0.774382    \n## days_deprived:Subject351 -13.1681     5.5703  -2.364 0.019867 *  \n## days_deprived:Subject352 -14.4019     5.5703  -2.585 0.011057 *  \n## days_deprived:Subject369  -7.8948     5.5703  -1.417 0.159273    \n## days_deprived:Subject370  -1.0495     5.5703  -0.188 0.850912    \n## days_deprived:Subject371  -9.3443     5.5703  -1.678 0.096334 .  \n## days_deprived:Subject372 -10.6041     5.5703  -1.904 0.059613 .  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 25.53 on 108 degrees of freedom\n## Multiple R-squared:  0.849,  Adjusted R-squared:  0.8001 \n## F-statistic: 17.35 on 35 and 108 DF,  p-value: < 2.2e-16\nall_intercepts <- c(coef(np_model)[\"(Intercept)\"],\n                    coef(np_model)[3:19] + coef(np_model)[\"(Intercept)\"])\n\nall_slopes  <- c(coef(np_model)[\"days_deprived\"],\n                 coef(np_model)[20:36] + coef(np_model)[\"days_deprived\"])\n\nids <- sleep2 %>% pull(Subject) %>% levels() %>% factor()\n\n# make a tibble with the data extracted above\nnp_coef <- tibble(Subject = ids,\n                  intercept = all_intercepts,\n                  slope = all_slopes)\n\nnp_coef## # A tibble: 18 × 3\n##    Subject intercept slope\n##    <fct>       <dbl> <dbl>\n##  1 308          288. 21.7 \n##  2 309          200.  4.36\n##  3 310          226.  3.90\n##  4 330          273.  8.01\n##  5 331          298.  4.87\n##  6 332          316.  2.40\n##  7 333          285. 10.9 \n##  8 334          238. 18.1 \n##  9 335          263. -4.21\n## 10 337          313. 22.4 \n## 11 349          229. 16.4 \n## 12 350          248. 23.3 \n## 13 351          264.  8.52\n## 14 352          331.  7.29\n## 15 369          267. 13.8 \n## 16 370          235. 20.6 \n## 17 371          258. 12.3 \n## 18 372          291. 11.1\nggplot(sleep2, aes(x = days_deprived, y = Reaction)) +\n  geom_abline(data = np_coef,\n              mapping = aes(intercept = intercept,\n                            slope = slope),\n              color = 'blue') +\n  geom_point() +\n  scale_x_continuous(breaks = 0:7) +\n  facet_wrap(~Subject) +\n  labs(y = \"Reaction Time\", x = \"Days deprived of sleep (0 = baseline)\")\nnp_coef %>% pull(slope) %>% t.test()## \n##  One Sample t-test\n## \n## data:  .\n## t = 6.1971, df = 17, p-value = 9.749e-06\n## alternative hypothesis: true mean is not equal to 0\n## 95 percent confidence interval:\n##   7.542244 15.328613\n## sample estimates:\n## mean of x \n##  11.43543"},{"path":"introducing-linear-mixed-effects-models.html","id":"partial-pooling-using-mixed-effects-models","chapter":"5 Introducing linear mixed-effects models","heading":"5.2.3 Partial pooling using mixed-effects models","text":"Neither complete -pooling approach satisfactory. desirable improve estimates individual participants taking advantage know participants. help us better distinguish signal error participant improve generalization population. web app show, becomes particularly important unbalanced missing data.-pooling model, treated Subject fixed factor. pair intercept slope estimates determined subject's data alone. However, interested 18 subjects ; rather, interested examples drawn larger population potential subjects. subjects--fixed-effects approach suboptimal goal generalize new participants population interest.Partial pooling happens treat factor random instead fixed analysis. random factor factor whose levels considered represent proper subset levels population. Usually, treat factor random levels data result sampling, want generalize beyond levels. case, eighteen unique subjects thus, eighteen levels Subject factor, like say something general effects sleep deprivation population potential subjects.way include random factors analysis use linear mixed-effects model. , estimates level factor (.e., subject) become informed information levels (.e., subjects). Rather estimating intercept slope participant without considering estimates subjects, model estimates values population, pulls estimates individual subjects toward values, statistical phenomenon known shrinkage.multilevel model . important understand math means. looks complicated first, really nothing seen . explain everything step step.Level 1:\\[\\begin{equation}\nY_{sd} = \\beta_{0s} + \\beta_{1s} X_{sd} + e_{sd}\n\\end{equation}\\]Level 2:\\[\\begin{equation}\n\\beta_{0s} = \\gamma_{0} + S_{0s}\n\\end{equation}\\]\\[\\begin{equation}\n\\beta_{1s} = \\gamma_{1} + S_{1s}\n\\end{equation}\\]Variance Components:\\[\\begin{equation}\n \\langle S_{0s}, S_{1s} \\rangle \\sim N\\left(\\langle 0, 0 \\rangle, \\mathbf{\\Sigma}\\right) \n\\end{equation}\\]\\[\\begin{equation}\n\\mathbf{\\Sigma} = \\left(\\begin{array}{cc}{\\tau_{00}}^2 & \\rho\\tau_{00}\\tau_{11} \\\\\n         \\rho\\tau_{00}\\tau_{11} & {\\tau_{11}}^2 \\\\\n         \\end{array}\\right) \n\\end{equation}\\]\\[\\begin{equation}\ne_{sd} \\sim N\\left(0, \\sigma^2\\right)\n\\end{equation}\\]case get lost, table explanation variables set equations .Note \"Status\" column table contains values fixed, random, derived. Although fixed random standard terms, derived ; introduced help think different variables mean context model help distinguish variables directly estimated variables .begin Level 1 equation model, represents general relationship predictors response variable. captures functional form main relationship reaction time \\(Y_{sd}\\) sleep deprivation \\(X_{sd}\\): straight line intercept \\(\\beta_{0s}\\) slope \\(\\beta_{1s}\\). Now \\(\\beta_{0s}\\) \\(\\beta_{1s}\\) makes look like complete pooling model, estimated single intercept single slope entire dataset; however, actually estimating directly. Instead, going think \\(\\beta_{0s}\\) \\(\\beta_{1s}\\) derived: wholly defined variables Level 2 model.Level 2 model, defined two equations, represents relationships participant level. , define intercept \\(\\beta_{0s}\\) terms fixed effect \\(\\gamma_0\\) random intercept \\(S_{0s}\\); likewise, define slope \\(\\beta_{1s}\\) terms fixed slope \\(\\gamma_1\\) random slope \\(S_{1s}\\).final equations represent Variance Components model. get detail .substitute Level 2 equations Level 1 see advantages representing things multilevel way.\\[\\begin{equation}\nY_{sd} = \\gamma_{0} + S_{0s} + \\left(\\gamma_{1} + S_{1s}\\right) X_{sd} + e_{sd}\n\\end{equation}\\]\"combined\" formula syntax easy enough understand particular case, multilevel form clearly allows us see functional form model: straight line. easily change functional form , instance, capture non-linear trends:\\[Y_{sd} = \\beta_{0s} + \\beta_{1s} X_{sd} + \\beta_{2s} X_{sd}^2 + e_{sd}\\]functional form gets obscured combined syntax. multilevel syntax also makes easy see terms go intercept terms go slope. Also, designs get compilicated---example, assign participants different experimental conditions, thus introducing predictors Level 2---combined equations get harder harder parse reason .Fixed effect parameters like \\(\\gamma_0\\) \\(\\gamma_1\\) estimated data, reflect stable properties population. example, \\(\\gamma_0\\) population intercept \\(\\gamma_1\\) population slope. can think fixed-effects parameters representing average intercept slope population. \"fixed\" sense assume reflect true underlying values population; assumed vary sample sample. fixed effects parameters often prime theoretical interest; want measure standard errors manner unbiased precise data allow. experimental settings often targets hypothesis tests.Random effects like \\(S_{0i}\\) \\(S_{1i}\\) allow intercepts slopes (respectively) vary subjects. random effects offsets: deviations population 'grand mean' values. subjects just slower responders others, higher intercept (mean RT) day 0 population's estimated value \\(\\hat{\\gamma_0}\\). slower--average subjects positive \\(S_{0i}\\) values; faster--average subjects negative \\(S_{0i}\\) values. Likewise, subjects show stronger effects sleep deprivation (steeper slope) estimated population effect, \\(\\hat{\\gamma_1}\\), implies positive offset \\(S_{1s}\\), others may show weaker effects close none (negative offset).participant can represented vector pair \\(\\langle S_{0i}, S_{1i} \\rangle\\). subjects sample comprised entire population, justified treating fixed estimating values, \"-pooling\" approach . case . recognition fact sampled, going treat subjects random factor rather fixed factor. Instead estimating values subjects happened pick, estimate covariance matrix represents bivariate distribution pairs values drawn. , allow subjects sample inform us characteristics population.","code":""},{"path":"introducing-linear-mixed-effects-models.html","id":"the-variance-covariance-matrix","chapter":"5 Introducing linear mixed-effects models","heading":"5.2.4 The variance-covariance matrix","text":"\\[\\begin{equation}\n \\langle S_{0s}, S_{1s} \\rangle \\sim N\\left(\\langle 0, 0 \\rangle, \\mathbf{\\Sigma}\\right) \n\\end{equation}\\]\\[\\begin{equation}\n\\mathbf{\\Sigma} = \\left(\\begin{array}{cc}{\\tau_{00}}^2 & \\rho\\tau_{00}\\tau_{11} \\\\\n         \\rho\\tau_{00}\\tau_{11} & {\\tau_{11}}^2 \\\\\n         \\end{array}\\right) \n\\end{equation}\\]Equations Variance Components characterize estimates variability. first equation states assumption random intercept / random slope pairs \\(\\langle S_{0s}, S_{1s} \\rangle\\) drawn bivariate normal distribution centered origin \\(\\langle 0, 0 \\rangle\\) variance-covariance matrix \\(\\mathbf{\\Sigma}\\).variance-covariance matrix key: determines probability drawing random effect pairs \\(\\langle S_{0s}, S_{1s} \\rangle\\) population. seen , chapter correlation regression. covariance matrix always square matrix (equal numbers columns rows). main diagonal (upper left bottom right cells) random effect variances \\({\\tau_{00}}^2\\) \\({\\tau_{11}}^2\\). \\({\\tau_{00}}^2\\) random intercept variance, captures much subjects vary mean response time Day 0, sleep deprivation. \\({\\tau_{11}}^2\\) random slope variance, captures much subjects vary susceptibility effects sleep deprivation.cells -diagonal contain covariances, information represented redundantly matrix; lower left element identical upper right element; capture covariance random intercepts slopes, expressed \\(\\rho\\tau_{00}\\tau_{11}\\). equation \\(\\rho\\) correlation intercept slope. , information matrix can captured just three parameters: \\(\\tau_{00}\\), \\(\\tau_{11}\\), \\(\\rho\\).","code":""},{"path":"introducing-linear-mixed-effects-models.html","id":"estimating-the-model-parameters","chapter":"5 Introducing linear mixed-effects models","heading":"5.3 Estimating the model parameters","text":"estimate parameters, going use lmer() function lme4 package (Bates, Mächler, Bolker, & Walker, 2015). basic syntax lmer() iswhere formula expresses structure underlying model compact format data data frame variables mentioned formula can found.general format model formula N fixed effects (fix) K random effects (ran) isDV ~ fix1 + fix2 + ... + fixN + (ran1 + ran2 + ... + ranK | random_factor1)Interactions factors B can specified using either * B (interaction main effects) :B (just interaction).key difference standard R model syntax presence random effect term, enclosed parentheses, e.g., (ran1 + ran2 + ... + ranK | random_factor). bracketed expression represents random effects associated single random factor. can one random effects term single formula, see talk crossed random factors. think random effects terms providing lmer() instructions construct variance-covariance matrices.left side bar | put effects want allow vary levels random factor named right side. Usually, right-side variable one whose values uniquely identify individual subjects (e.g., subject_id).Consider following possible model formulas sleep2 data variance-covariance matrices construct.Model 1:\\[\\begin{equation*}\n  \\mathbf{\\Sigma} = \\left(\n  \\begin{array}{cc}\n    {\\tau_{00}}^2 & 0 \\\\\n                0 & 0 \\\\\n  \\end{array}\\right) \n\\end{equation*}\\]Models 2 3:\\[\\begin{equation*}\n  \\mathbf{\\Sigma} = \\left(\n  \\begin{array}{cc}\n             {\\tau_{00}}^2 & \\rho\\tau_{00}\\tau_{11} \\\\\n    \\rho\\tau_{00}\\tau_{11} &          {\\tau_{11}}^2 \\\\\n  \\end{array}\\right) \n\\end{equation*}\\]Model 4:\\[\\begin{equation*}\n  \\mathbf{\\Sigma} = \\left(\n  \\begin{array}{cc}\n    0 &             0 \\\\\n    0 & {\\tau_{11}}^2 \\\\\n  \\end{array}\\right) \n\\end{equation*}\\]Model 5:\\[\\begin{equation*}\n  \\mathbf{\\Sigma} = \\left(\n  \\begin{array}{cc}\n    {\\tau_{00}}^2 &             0 \\\\\n                0 & {\\tau_{11}}^2 \\\\\n  \\end{array}\\right) \n\\end{equation*}\\]reasonable model data Model 2, stick .fit model, storing result object pp_mod.discussing interpret output, first plot data model predictions. can get model predictions using predict() function (see ?predict.merMod information use mixed-effects models).First, create new data frame predictor values Subject days_deprived., run predict(). Typically add prediction new variable data frame new data, giving name DV (Reaction).Now ready plot.\nFigure 5.6: Data plotted predictions partial pooling approach.\n","code":"lmer(formula, data, ...)\npp_mod <- lmer(Reaction ~ days_deprived + (days_deprived | Subject), sleep2)\n\nsummary(pp_mod)## Linear mixed model fit by REML ['lmerMod']\n## Formula: Reaction ~ days_deprived + (days_deprived | Subject)\n##    Data: sleep2\n## \n## REML criterion at convergence: 1404.1\n## \n## Scaled residuals: \n##     Min      1Q  Median      3Q     Max \n## -4.0157 -0.3541  0.0069  0.4681  5.0732 \n## \n## Random effects:\n##  Groups   Name          Variance Std.Dev. Corr\n##  Subject  (Intercept)   958.35   30.957       \n##           days_deprived  45.78    6.766   0.18\n##  Residual               651.60   25.526       \n## Number of obs: 144, groups:  Subject, 18\n## \n## Fixed effects:\n##               Estimate Std. Error t value\n## (Intercept)    267.967      8.266  32.418\n## days_deprived   11.435      1.845   6.197\n## \n## Correlation of Fixed Effects:\n##             (Intr)\n## days_deprvd -0.062\nnewdata <- crossing(\n  Subject = sleep2 %>% pull(Subject) %>% levels() %>% factor(),\n  days_deprived = 0:7)\n\nhead(newdata, 17)## # A tibble: 17 × 2\n##    Subject days_deprived\n##    <fct>           <int>\n##  1 308                 0\n##  2 308                 1\n##  3 308                 2\n##  4 308                 3\n##  5 308                 4\n##  6 308                 5\n##  7 308                 6\n##  8 308                 7\n##  9 309                 0\n## 10 309                 1\n## 11 309                 2\n## 12 309                 3\n## 13 309                 4\n## 14 309                 5\n## 15 309                 6\n## 16 309                 7\n## 17 310                 0\nnewdata2 <- newdata %>%\n  mutate(Reaction = predict(pp_mod, newdata))\nggplot(sleep2, aes(x = days_deprived, y = Reaction)) +\n  geom_line(data = newdata2,\n            color = 'blue') +\n  geom_point() +\n  scale_x_continuous(breaks = 0:7) +\n  facet_wrap(~Subject) +\n  labs(y = \"Reaction Time\", x = \"Days deprived of sleep (0 = baseline)\")"},{"path":"introducing-linear-mixed-effects-models.html","id":"interpreting-lmer-output-and-extracting-estimates","chapter":"5 Introducing linear mixed-effects models","heading":"5.4 Interpreting lmer() output and extracting estimates","text":"call lmer() returns fitted model object class \"lmerMod\". find lmerMod class, turn specialized version merMod class, see ?lmerMod-class.","code":""},{"path":"introducing-linear-mixed-effects-models.html","id":"fixed-effects","chapter":"5 Introducing linear mixed-effects models","heading":"5.4.1 Fixed effects","text":"section output called Fixed effects: look familiar; similar see output simple linear model fit lm().indicates estimated mean reaction time participants Day 0 \n268 milliseconds,\nday sleep deprivation adding additional\n11 milliseconds\nresponse time, average.need get fixed effects model, can extract using fixef().standard errors give us estimates variability parameters due sampling error. use calculate \\(t\\)-values derive confidence intervals. Extract using vcov(pp_mod) gives variance-covariance matrix (one associated random effects), pull diagonal using diag() take square root using sqrt().Note \\(t\\) values appear \\(p\\) values, customary simpler modeling frameworks. multiple approaches getting \\(p\\) values mixed-effects models, advantages disadvantages ; see Luke (2017) survey options. \\(t\\) values appear degrees freedom, degrees freedom mixed-effects model well-defined. Often people treat Wald \\(z\\) values, .e., observations standard normal distribution. \\(t\\) distribution asymptotes standard normal distribution number observations goes infinity, \"t--z\" practice legitimate large enough set observations.calculate Wald \\(z\\) values, just divide fixed effect estimate standard error:can get associated \\(p\\)-values using following formula:gives us strong evidence null hypothesis \\(H_0: \\gamma_1 = 0\\). Sleep deprivation appear increase response time.can get confidence intervals estimates using confint() (technique uses parametric bootstrap). confint() generic function, get help function, use ?confint.merMod.","code":"## Fixed effects:\n##               Estimate Std. Error t value\n## (Intercept)    267.967      8.266  32.418\n## days_deprived   11.435      1.845   6.197\nfixef(pp_mod)##   (Intercept) days_deprived \n##     267.96742      11.43543\nsqrt(diag(vcov(pp_mod)))\n\n# OR, equivalently using pipes:\n# vcov(pp_mod) %>% diag() %>% sqrt()##   (Intercept) days_deprived \n##      8.265896      1.845293\ntvals <- fixef(pp_mod) / sqrt(diag(vcov(pp_mod)))\n\ntvals##   (Intercept) days_deprived \n##     32.418437      6.197082\n2 * (1 - pnorm(abs(tvals)))##   (Intercept) days_deprived \n##   0.00000e+00   5.75197e-10\nconfint(pp_mod)## Computing profile confidence intervals ...##                     2.5 %      97.5 %\n## .sig01         19.0979934  46.3366599\n## .sig02         -0.4051073   0.8058951\n## .sig03          4.0079284  10.2487351\n## .sigma         22.4666029  29.3494509\n## (Intercept)   251.3443396 284.5904989\n## days_deprived   7.7245247  15.1463328"},{"path":"introducing-linear-mixed-effects-models.html","id":"random-effects","chapter":"5 Introducing linear mixed-effects models","heading":"5.4.2 Random effects","text":"random effects part summary() output less familiar. find table information variance components: variance-covariance matrix (matrices, multiple random factors) residual variance.start Residual line. tells us residual variance, \\(\\sigma^2\\), estimated \n651.6. value next column,\n25.526, just standard deviation, \\(\\sigma\\), square root variance.extract residual standard deviation using sigma() function.two lines Residual line give us information variance-covariance matrix Subject random factor.values Variance column gives us main diagonal matrix, Std.Dev. values just square roots values. Corr column tells us correlation intercept slope.can extract values fitted object pp_mod using VarCorr() function. returns named list, one element random factor. Subject random factor, list just length 1.first lines printout variance covariance matrix. can see variances main diagonal. can get :can get correlation intecepts slopes two ways. First, extracting \"correlation\" attribute pulling element row 1 column 2 ([1, 2]):can directly compute value variance-covariance matrix .can pull estimated random effects (BLUPS) using ranef(). Like VarCorr() , result named list, element corresponding single random factor.extractor functions useful. See ?merMod-class details.can get fitted values model using fitted() residuals using residuals(). (functions take account \"conditional modes random effects\", .e., BLUPS).Finally, can get predictions new data using predict(), . use predict() imagine might happened continued study three extra days.\nFigure 5.7: Data model extrapolation.\n","code":"## Random effects:\n##  Groups   Name          Variance Std.Dev. Corr\n##  Subject  (Intercept)   958.35   30.957       \n##           days_deprived  45.78    6.766   0.18\n##  Residual               651.60   25.526       \n## Number of obs: 144, groups:  Subject, 18\nsigma(pp_mod) # residual## [1] 25.5264##  Groups   Name          Variance Std.Dev. Corr\n##  Subject  (Intercept)   958.35   30.957       \n##           days_deprived  45.78    6.766   0.18\n# variance-covariance matrix for random factor Subject\nVarCorr(pp_mod)[[\"Subject\"]] # equivalently: VarCorr(pp_mod)[[1]]##               (Intercept) days_deprived\n## (Intercept)      958.3517      37.20460\n## days_deprived     37.2046      45.77766\n## attr(,\"stddev\")\n##   (Intercept) days_deprived \n##     30.957255      6.765919 \n## attr(,\"correlation\")\n##               (Intercept) days_deprived\n## (Intercept)     1.0000000     0.1776263\n## days_deprived   0.1776263     1.0000000\ndiag(VarCorr(pp_mod)[[\"Subject\"]]) # just the variances##   (Intercept) days_deprived \n##     958.35165      45.77766\nattr(VarCorr(pp_mod)[[\"Subject\"]], \"correlation\")[1, 2] # the correlation## [1] 0.1776263\n# directly compute correlation from variance-covariance matrix\nmx <- VarCorr(pp_mod)[[\"Subject\"]]\n\n## if cov = rho * t00 * t11, then\n## rho = cov / (t00 * t11).\nmx[1, 2] / (sqrt(mx[1, 1]) * sqrt(mx[2, 2]))## [1] 0.1776263\nranef(pp_mod)[[\"Subject\"]]##     (Intercept) days_deprived\n## 308  24.4992891     8.6020000\n## 309 -59.3723102    -8.1277534\n## 310 -39.4762764    -7.4292365\n## 330   1.3500428    -2.3845976\n## 331  18.4576169    -3.7477340\n## 332  30.5270040    -4.8936899\n## 333  13.3682027     0.2888639\n## 334 -18.1583020     3.8436686\n## 335 -16.9737887   -12.0702333\n## 337  44.5850842    10.1760837\n## 349 -26.6839022     2.1946699\n## 350  -5.9657957     8.1758613\n## 351  -5.5710355    -2.3718494\n## 352  46.6347253    -0.5616377\n## 369   0.9616395     1.7385130\n## 370 -18.5216778     5.6317534\n## 371  -7.3431320     0.2729282\n## 372  17.6826159     0.6623897\nmutate(sleep2,\n       fit = fitted(pp_mod),\n       resid = residuals(pp_mod)) %>%\n  group_by(Subject) %>%\n  slice(c(1,10)) %>%\n  print(n = +Inf)## # A tibble: 18 × 6\n## # Groups:   Subject [18]\n##    Reaction  Days Subject days_deprived   fit  resid\n##       <dbl> <dbl> <fct>           <dbl> <dbl>  <dbl>\n##  1     251.     2 308                 0  292. -41.7 \n##  2     203.     2 309                 0  209.  -5.62\n##  3     234.     2 310                 0  228.   5.83\n##  4     284.     2 330                 0  269.  14.5 \n##  5     302.     2 331                 0  286.  15.4 \n##  6     273.     2 332                 0  298. -25.5 \n##  7     277.     2 333                 0  281.  -4.57\n##  8     243.     2 334                 0  250.  -6.44\n##  9     254.     2 335                 0  251.   3.50\n## 10     292.     2 337                 0  313. -20.9 \n## 11     239.     2 349                 0  241.  -2.36\n## 12     256.     2 350                 0  262.  -5.80\n## 13     270.     2 351                 0  262.   7.50\n## 14     327.     2 352                 0  315.  12.3 \n## 15     257.     2 369                 0  269. -11.7 \n## 16     239.     2 370                 0  249. -10.5 \n## 17     278.     2 371                 0  261.  17.3 \n## 18     298.     2 372                 0  286.  11.9\n## create the table with new predictor values\nndat <- crossing(Subject = sleep2 %>% pull(Subject) %>% levels() %>% factor(),\n                 days_deprived = 8:10) %>%\n  mutate(Reaction = predict(pp_mod, newdata = .))\nggplot(sleep2, aes(x = days_deprived, y = Reaction)) +\n  geom_line(data = bind_rows(newdata2, ndat),\n            color = 'blue') +\n  geom_point() +\n  scale_x_continuous(breaks = 0:10) +\n  facet_wrap(~Subject) +\n  labs(y = \"Reaction Time\", x = \"Days deprived of sleep (0 = baseline)\")"},{"path":"introducing-linear-mixed-effects-models.html","id":"multi-level-app","chapter":"5 Introducing linear mixed-effects models","heading":"5.5 Multi-level app","text":"Try multi-level web app sharpen understanding three different approaches multi-level modeling.","code":""},{"path":"linear-mixed-effects-models-with-one-random-factor.html","id":"linear-mixed-effects-models-with-one-random-factor","chapter":"6 Linear mixed-effects models with one random factor","heading":"6 Linear mixed-effects models with one random factor","text":"","code":""},{"path":"linear-mixed-effects-models-with-one-random-factor.html","id":"when-and-why-would-you-want-to-replace-conventional-analyses-with-linear-mixed-effects-modeling","chapter":"6 Linear mixed-effects models with one random factor","heading":"6.1 When, and why, would you want to replace conventional analyses with linear mixed-effects modeling?","text":"repeatedly emphasized many common techniques psychology can seen special cases general linear model. implies possible replace techniques regression. fact, analyze almost conceivable dataset psychology using one four functions .end chapter, :understand replace standard ANOVA t-test analyses regression data single random factor continuous DV;able perform model comparison (anova()) testing effects;able express various study designs using R regression formula syntax.decide four regression functions use, need able answer two questions.type data dependent variable represent distributed?data multilevel single level?Arguments functions highly similar across four versions. learn analyzing count categorical data later course. now, focus continuous data, principles generalize types data.comparison chart single-level data (data repeated-measures):designs -subjects designs without repeated measures. (Note factorial case, probably replace b deviation-coded numerical predictors, reasons already discussed chapter interactions).mixed-effects models come play multilevel data. Data usually multilevel one three reasons (multiple reasons simultaneously apply):within-subject factor, /oryou pseudoreplications, /oryou multiple stimulus items (discuss next chapter).(point, good idea refresh memory meaning - versus within- subject factors). sleepstudy data, within-subject factor Day (numeric variable, actually, factor; multiple values varying within participant).Pseudoreplications occur take multiple measurements within condition. instance, imagine study randomly assign participants consume one two beverages---alcohol water---administering simple response time task press button fast possible light flashes. probably take one measurement response time participant; assume measured 100 trials. one -subject factor (beverage) 100 observations per subject, say, 20 subjects group. One common mistake novices make analyzing data try run t-test. directly use conventional t-test pseudoreplications (multiple stimuli). must first calculate means subject, run analysis means, raw data. versions ANOVA can deal pseudoreplications, probably better using linear-mixed effects model, can better handle complex dependency structure.comparison chart multi-level data:One main selling points general linear models / regression framework t-test ANOVA flexibility. saw last chapter sleepstudy data, properly handled within linear mixed-effects modelling framework. Despite many advantages regression, situation balanced data can reasonably apply t-test ANOVA without violating assumptions behind test, makes sense ; approaches long history psychology widely understood.","code":""},{"path":"linear-mixed-effects-models-with-one-random-factor.html","id":"example-independent-samples-t-test-on-multi-level-data","chapter":"6 Linear mixed-effects models with one random factor","heading":"6.2 Example: Independent-samples \\(t\\)-test on multi-level data","text":"consider situation testing effect alcohol consumption simple reaction time (e.g., press button fast can light appears). keep simple, assume collected data 14 participants randomly assigned perform set 10 simple RT trials one two interventions: drinking pint alcohol (treatment condition) placebo drink (placebo condition). 7 participants two groups. Note need real study.web app presents simulated data study. Subjects P01-P07 placebo condition, subjects T01-T07 treatment condition. Please stop look!going run t-test data, first need calculate subject means, otherwise observations independent. follows. (want run code , can download sample data web app save independent_samples.csv)., \\(t\\)-test can run using \"formula\" version t.test().nothing wrong analysis, aggregating data throws away information. can see web app actually two different sources variability: trial--trial variability simple RT (represented \\(\\sigma\\)) variability across subjects terms slow fast relative population mean (\\(\\gamma_{00}\\)). Data Generating Process response time (\\(Y_{st}\\)) subject \\(s\\) trial \\(t\\) shown .Level 1:\\[\\begin{equation}\nY_{st} = \\beta_{0s} + \\beta_{1} X_{s} + e_{st}\n\\end{equation}\\]Level 2:\\[\\begin{equation}\n\\beta_{0s} = \\gamma_{00} + S_{0s}\n\\end{equation}\\]\\[\\begin{equation}\n\\beta_{1} = \\gamma_{10}\n\\end{equation}\\]Variance Components:\\[\\begin{equation}\nS_{0s} \\sim N\\left(0, {\\tau_{00}}^2\\right) \n\\end{equation}\\]\\[\\begin{equation}\ne_{st} \\sim N\\left(0, \\sigma^2\\right)\n\\end{equation}\\]equation, \\(X_s\\) numerical predictor coding condition subject \\(s\\) ; e.g., 0 placebo, 1 treatment.multi-level equations somewhat cumbersome simple model; just reduce levels 1 2 \\[\\begin{equation}\nY_{st} = \\gamma_{00} + S_{0s} + \\gamma_{10} X_s + e_{st},\n\\end{equation}\\]worth becoming familiar multi-level format encounter complex designs.Unlike sleepstudy data seen last chapter, one random effect subject, \\(S_{0s}\\). random slope. subject appears one two treatment conditions, possible estimate effect placebo versus alcohol varies subjects. mixed-effects model fit data, random intercepts random slopes, known random intercepts model.random-intercepts model adequately capture two sources variability mentioned : inter-subject variability overall mean RT parameter \\({\\tau_{00}}^2\\), trial--trial variability parameter \\(\\sigma^2\\). can calculate proportion total variability attributable individual differences among subjects using formula .\\[ICC = \\frac{{\\tau_{00}}^2}{{\\tau_{00}}^2 + \\sigma^2}\\]quantity, known intra-class correlation coefficient, tells much clustering data. ranges 0 1, 0 indicating variability due residual variance, 1 indicating variability due individual differences among subjects.lmer syntax fitting random intercepts model data lmer(RT ~ cond + (1 | subject), dat, REML=FALSE). create numerical predictor first, make explicit using dummy coding.now, estimate model.Play around sliders app check lmer output panel understand output maps onto model parameters.","code":"\nlibrary(\"tidyverse\")\n\ndat <- read_csv(\"data/independent_samples.csv\", col_types = \"cci\")\n\nsubj_means <- dat %>%\n  group_by(subject, cond) %>%\n  summarise(mean_rt = mean(RT)) %>%\n  ungroup()\n\nsubj_means## # A tibble: 14 × 3\n##    subject cond  mean_rt\n##    <chr>   <chr>   <dbl>\n##  1 P01     P        354 \n##  2 P02     P        384.\n##  3 P03     P        391.\n##  4 P04     P        404.\n##  5 P05     P        421.\n##  6 P06     P        392 \n##  7 P07     P        400.\n##  8 T08     T        430.\n##  9 T09     T        432.\n## 10 T10     T        410.\n## 11 T11     T        455.\n## 12 T12     T        450.\n## 13 T13     T        418.\n## 14 T14     T        489.\nt.test(mean_rt ~ cond, subj_means)## \n##  Welch Two Sample t-test\n## \n## data:  mean_rt by cond\n## t = -3.7985, df = 11.32, p-value = 0.002807\n## alternative hypothesis: true difference in means between group P and group T is not equal to 0\n## 95 percent confidence interval:\n##  -76.32580 -20.44563\n## sample estimates:\n## mean in group P mean in group T \n##        392.3143        440.7000\ndat2 <- dat %>%\n  mutate(cond_d = if_else(cond == \"T\", 1L, 0L))\n\ndistinct(dat2, cond, cond_d)  ## double check## # A tibble: 2 × 2\n##   cond  cond_d\n##   <chr>  <int>\n## 1 P          0\n## 2 T          1\nlibrary(\"lme4\")\n\nmod <- lmer(RT ~ cond_d + (1 | subject), dat2, REML = FALSE)\n\nsummary(mod)## Linear mixed model fit by maximum likelihood  ['lmerMod']\n## Formula: RT ~ cond_d + (1 | subject)\n##    Data: dat2\n## \n##      AIC      BIC   logLik deviance df.resid \n##   1451.8   1463.5   -721.9   1443.8      136 \n## \n## Scaled residuals: \n##      Min       1Q   Median       3Q      Max \n## -2.67117 -0.66677  0.01656  0.75361  2.58447 \n## \n## Random effects:\n##  Groups   Name        Variance Std.Dev.\n##  subject  (Intercept)  329.3   18.15   \n##  Residual             1574.7   39.68   \n## Number of obs: 140, groups:  subject, 14\n## \n## Fixed effects:\n##             Estimate Std. Error t value\n## (Intercept)  392.314      8.339  47.045\n## cond_d        48.386     11.793   4.103\n## \n## Correlation of Fixed Effects:\n##        (Intr)\n## cond_d -0.707"},{"path":"linear-mixed-effects-models-with-one-random-factor.html","id":"when-is-a-random-intercepts-model-appropriate","chapter":"6 Linear mixed-effects models with one random factor","heading":"6.2.1 When is a random-intercepts model appropriate?","text":"course, mixed-effects model appropriate multilevel data. random-intercepts model appropriate one-sample -subjects data pseudoreplications (pseudoreplications situation, multilevel data, can just use vanilla regression, e.g., lm()).\"random-intercepts-\" model also appropriate within-subject factors design, also pseudoreplications; , appropriate single observation per subject per level within-subject factor. one observation per subject per level/cell, need enrich random effects structure random slopes, described next section. reason multiple observations per subject per level subject reacting set stimuli, might want consider mixed-effects model crossed random effects subjects stimuli, described next chapter.logic goes factorial designs one within-subjects factor. factorial designs, random-intercepts model appropriate one observation per subject per cell formed combination within-subjects factors. instance, \\(\\) \\(B\\) two two-level within-subject factors, need check one observation subject \\(A_1B_1\\), \\(A_1B_2\\), \\(A_2B_1\\), \\(A_2B_2\\). one observation, need consider including random slope model.","code":""},{"path":"linear-mixed-effects-models-with-one-random-factor.html","id":"expressing-the-study-design-and-performing-tests-in-regression","chapter":"6 Linear mixed-effects models with one random factor","heading":"6.3 Expressing the study design and performing tests in regression","text":"order reproduce t-test/ANOVA-style analyses linear mixed-effects models, need better understand two things: (1) express study design regression formula, (2) get p-values tests perform. latter part obvious within linear mixed-effects approach, since many circumstances, output lme4::lmer() give p-values default, reflecting fact multiple options deriving (Luke, 2017). , might get p-values individual regression coefficients, test want perform composite one need test multiple parameters simultaneously.First, take closer look regression formula syntax R lme4. regression model \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_m x_m + e\\) -subject random effects :y ~ 1 + x1 + x2 + ... + xm + (1 + x1 + ... | subject)residual term implied mentioned; predictors. left side (tilde ~) specifies response variable, right side predictor variables. term brackets, (... | ...), specific lme4. right side vertical bar | specifies name variable (case, subject) codes levels random factor. left side specifies regression coefficients want allow vary levels. 1 start formula specifies want intercept, included default anyway, can omitted. 1 within bracket specifies random intercept, also included default; predictor variables mentioned inside brackets specify random slopes. write formula equivalently :y ~ x1 + x2 + ... + xm + (x1 + ... | subject).another important type shortcut R formulas, already saw chapter interactions; namely \"star syntax\" * b specifying interactions. two factors, b design, want main effects interactions model, can use:y ~ * bwhich equivalent y ~ + b + :b b predictors coding main effects B respectively, :b codes AB interaction. saves lot typing (mistakes) use star syntax interactions rather spelling . can seen easily example , codes 2x2x2 factorial design including factors , B, C:y ~ * b * cwhich equivalent toy ~ + b + c + :b + :c + b:c + :b:cwhere :b, :c, b:c two-way interactions :b:c three-way interaction. can use star syntax inside brackets random effects term well, e.g.,y ~ * b * c + (* b * c | subject)equivalent toy ~ + b + c + :b + :c + b:c + :b:c + (+ b + c + :b + :c + b:c + :b:c | subject), despite complexity, design uncommon psychology (e.g., factors within multiple stimuli per condition)!","code":""},{"path":"linear-mixed-effects-models-with-one-random-factor.html","id":"factors-with-more-than-two-levels","chapter":"6 Linear mixed-effects models with one random factor","heading":"6.3.1 Factors with more than two levels","text":"noted , conduct one-factor ANOVA regression using formula lm(y ~ x) single-level lmer(y ~ x + (1 | subject)) multilevel version without pseudoreplications. formula, x predictor type factor(), R (default) convert \\(k-1\\) dummy-coded numerical predictors (one level; see interaction chapter information).often sensible code numerical predictors, particularly goal ANOVA-style tests main effects interactions, can difficult variables type factor. design one three-level factor called meal (breakfast, lunch, dinner) create two variables, lunch_v_breakfast dinner_v_breakfast following scheme .dependent variable calories, model :calories ~ lunch_v_breakfast + dinner_v_breakfast.wanted interact meal another two-level factor---say, time_of_week (weekday versus weekend, coded -.5 +.5 respectively), think calories consumed per meal differ across levels variable? model :calories ~ (lunch_v_breakfast + dinner_v_breakfast) * time_of_week.(inclusion interaction reason chose \"deviation\" coding scheme.). put brackets around predictors associated two-level variable one interacts time_of_week. star syntax shorthand :calories ~ lunch_v_breakfast + dinner_v_breakfast + time_of_week + lunch_v_breakfast:time_of_week + dinner_v_breakfast:time_of_week.\"regression way\" estimating parameters 3x2 factorial design.","code":""},{"path":"linear-mixed-effects-models-with-one-random-factor.html","id":"multiparameter-tests","chapter":"6 Linear mixed-effects models with one random factor","heading":"6.3.2 Multiparameter tests","text":"Whenever dealing designs categorical factors two levels, test regression coefficient associated given factor equivalent test effect ANOVA framework, provided use sum deviation coding. example, 3x2 design, two predictor variables coding main effect meal (lunch_v_breakfast dinner_v_breakfast). simulate data run one-factor ANOVA aov(), replicate analysis using regression function lm() (note procedure works mixed-effects models multilevel data, just using lme4::lmer() instead base::lm()).get three \\(F\\)-tests, one main effect (meal time_of_week) one interaction. happens fit model using lm()?OK, output looks different! perform ANOVA-like tests situations? estimates lunch_v_breakfast dinner_v_breakfast convert single test main effect meal? Likewise, two interaction terms, lunch_v_breakfast:tow dinner_v_breakfast:tow; convert single test interaction?solution perform multiparameter tests using model comparison, implemented anova() function R. test main effect meal, compare model containing two predictor variables coding factor (lunch_v_breakfast dinner_v_breakfast) model excluding two predictors otherwise identical. can either re-fit model writing excluding terms, using update() function removing terms (shortcut). write first.OK, now equivalent version using shortcut update() function, takes model want update first argument special syntax formula including changes. formula, use . ~ . -lunch_v_breakfast -dinner_v_breakfast~ Although formula seems weird, dot . says \"keep everything side model formula (left ~) original model.\" formula . ~ . use formula original model; , update(mod, . ~ .) fit exact model . contrast, . ~ . -x -y means \"everything left side (DV), remove variables x y right side.can see, gives us result .want test main effect time_of_week, remove predictor.Try figure test interaction .got exact results model comparison lm() using aov(). Although involved steps, worth learning approach ultimately give much flexibility.","code":"\n## make up some data\nset.seed(62)\nmeals <- tibble(meal = factor(rep(c(\"breakfast\", \"lunch\", \"dinner\"),\n                                  each = 6)),\n                time_of_week = factor(rep(rep(c(\"weekday\", \"weekend\"),\n                                              each = 3), 3)),\n                calories = rnorm(18, 450, 50))\n\n## use sum coding instead of default 'dummy' (treatment) coding\noptions(contrasts = c(unordered = \"contr.sum\", ordered = \"contr.poly\"))\n\naov(calories ~ meal * time_of_week, data = meals) %>%\n  summary()##                   Df Sum Sq Mean Sq F value Pr(>F)\n## meal               2   2164    1082   0.380  0.692\n## time_of_week       1   5084    5084   1.783  0.207\n## meal:time_of_week  2   4767    2384   0.836  0.457\n## Residuals         12  34209    2851\n## add our own numeric predictors\nmeals2 <- meals %>%\n  mutate(lunch_v_breakfast = if_else(meal == \"lunch\", 2/3, -1/3),\n         dinner_v_breakfast = if_else(meal == \"dinner\", 2/3, -1/3),\n         time_week = if_else(time_of_week == \"weekend\", 1/2, -1/2))\n\n## double check our coding\ndistinct(meals2, meal, time_of_week,\n         lunch_v_breakfast, dinner_v_breakfast, time_week)\n\n## fit regression model\nmod <- lm(calories ~ (lunch_v_breakfast + dinner_v_breakfast) *\n            time_week, data = meals2)\n\nsummary(mod)## # A tibble: 6 × 5\n##   meal      time_of_week lunch_v_breakfast dinner_v_breakfast time_week\n##   <fct>     <fct>                    <dbl>              <dbl>     <dbl>\n## 1 breakfast weekday                 -0.333             -0.333      -0.5\n## 2 breakfast weekend                 -0.333             -0.333       0.5\n## 3 lunch     weekday                  0.667             -0.333      -0.5\n## 4 lunch     weekend                  0.667             -0.333       0.5\n## 5 dinner    weekday                 -0.333              0.667      -0.5\n## 6 dinner    weekend                 -0.333              0.667       0.5\n## \n## Call:\n## lm(formula = calories ~ (lunch_v_breakfast + dinner_v_breakfast) * \n##     time_week, data = meals2)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -68.522 -35.895  -4.063  42.061  73.081 \n## \n## Coefficients:\n##                              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)                    451.15      12.58  35.848 1.42e-13 ***\n## lunch_v_breakfast              -25.23      30.83  -0.818    0.429    \n## dinner_v_breakfast             -20.59      30.83  -0.668    0.517    \n## time_week                       33.61      25.17   1.335    0.207    \n## lunch_v_breakfast:time_week    -58.31      61.65  -0.946    0.363    \n## dinner_v_breakfast:time_week    17.93      61.65   0.291    0.776    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 53.39 on 12 degrees of freedom\n## Multiple R-squared:  0.2599, Adjusted R-squared:  -0.04843 \n## F-statistic: 0.843 on 5 and 12 DF,  p-value: 0.5447\n## fit the model\nmod_main_eff <- lm(calories ~ time_week +\n                     lunch_v_breakfast:time_week + dinner_v_breakfast:time_week,\n                   meals2)\n\n## compare models\nanova(mod, mod_main_eff)## Analysis of Variance Table\n## \n## Model 1: calories ~ (lunch_v_breakfast + dinner_v_breakfast) * time_week\n## Model 2: calories ~ time_week + lunch_v_breakfast:time_week + dinner_v_breakfast:time_week\n##   Res.Df   RSS Df Sum of Sq      F Pr(>F)\n## 1     12 34209                           \n## 2     14 36373 -2   -2163.9 0.3795 0.6921\nmod_main_eff2 <- update(mod, . ~ . -lunch_v_breakfast -dinner_v_breakfast)\n\nanova(mod, mod_main_eff2)## Analysis of Variance Table\n## \n## Model 1: calories ~ (lunch_v_breakfast + dinner_v_breakfast) * time_week\n## Model 2: calories ~ time_week + lunch_v_breakfast:time_week + dinner_v_breakfast:time_week\n##   Res.Df   RSS Df Sum of Sq      F Pr(>F)\n## 1     12 34209                           \n## 2     14 36373 -2   -2163.9 0.3795 0.6921\nmod_tow <- update(mod, . ~ . -time_week)\n\nanova(mod, mod_tow)## Analysis of Variance Table\n## \n## Model 1: calories ~ (lunch_v_breakfast + dinner_v_breakfast) * time_week\n## Model 2: calories ~ lunch_v_breakfast + dinner_v_breakfast + lunch_v_breakfast:time_week + \n##     dinner_v_breakfast:time_week\n##   Res.Df   RSS Df Sum of Sq      F Pr(>F)\n## 1     12 34209                           \n## 2     13 39294 -1   -5084.1 1.7834 0.2065\nmod_interact <- update(mod, . ~ . -lunch_v_breakfast:time_week\n                       -dinner_v_breakfast:time_week)\n\nanova(mod, mod_interact)## Analysis of Variance Table\n## \n## Model 1: calories ~ (lunch_v_breakfast + dinner_v_breakfast) * time_week\n## Model 2: calories ~ lunch_v_breakfast + dinner_v_breakfast + time_week\n##   Res.Df   RSS Df Sum of Sq      F Pr(>F)\n## 1     12 34209                           \n## 2     14 38977 -2   -4767.4 0.8362 0.4571"},{"path":"linear-mixed-effects-models-with-crossed-random-factors.html","id":"linear-mixed-effects-models-with-crossed-random-factors","chapter":"7 Linear mixed-effects models with crossed random factors","heading":"7 Linear mixed-effects models with crossed random factors","text":"","code":""},{"path":"linear-mixed-effects-models-with-crossed-random-factors.html","id":"generalizing-over-encounters-between-subjects-and-stimuli","chapter":"7 Linear mixed-effects models with crossed random factors","heading":"7.1 Generalizing over encounters between subjects and stimuli","text":"common goal experiments psychology test claims behavior arises response certain types stimuli (sometimes, neural underpinning behavior). stimuli might , instance, words, images, sounds, videos, stories. examples claims might want test :listening words second language, bilinguals experience interference words native language?people rate attractiveness faces differently good mood bad mood?viewing soothing images help reduce stress relative neutral images?reading scenario ambiguously describing target individual, people likely make assumptions social group target belongs subliminally primed?One thing note claims type, \"happens measurements individual type X encounters stimulus type Y\", X drawn target population subjects Y drawn target population stimuli. words, attempt make generalizable claims particular class events involving encounters sampling units subjects stimuli (Barr, 2017). just like sample possible subjects target population subjects, also sample possible stimuli target population stimuli. Thus, drawing inferences, need account uncertainty introduced estimates sampling processes (Clark, 1973; Coleman, 1964; Judd et al., 2012; Yarkoni, 2019). Linear mixed-effects models make particularly easy allowing one random factor model formula (Baayen et al., 2008).simple example study interested testing whether people rate pictures cats, dogs, sunsets soothing images look . want say something general category cats, dogs, sunsets something specific pictures happened sample. say randomly select four images three categories Google Images (absolutely need able say something generalizable, chose small number keep example simple). table stimuli might look like following:sample set four participants perform soothing ratings. , four real study, keeping small just expository purposes.Now, subject given \"soothingness\" rating picture, full dataset consisting levels subject_id crossed levels stimulus_id. mean talk \"crossed random factors.\" can create table containing combinations crossing() function tidyr (loaded load tidyverse).4 subjects responding 12 stimuli, resulting table 48 rows.","code":"\ncrossing(subjects %>% select(subject_id),\n         stimuli %>% select(-category)) "},{"path":"linear-mixed-effects-models-with-crossed-random-factors.html","id":"lme4-syntax-for-crossed-random-factors","chapter":"7 Linear mixed-effects models with crossed random factors","heading":"7.2 lme4 syntax for crossed random factors","text":"analyze data? Recall last chapter lme4 formula syntax model -subject random intercepts slopes predictor x given y ~ x + (1 + x | subject_id) term brackets vertical bar | provides random effects specification. variable right bar, subject_id, specifies variable identifies levels random factor. formula left bar within brackets, 1 + x, specifies random effects associated factor, case random intercept random slope x. best way think bracketed part formula (1 + x | subject_id) instruction lme4::lmer() build covariance matrix capturing variance introduced random factor subjects. now realize instruction result estimation two-dimensional covariance matrix, one dimension intercept variance one slope variance.limited estimation random effects subjects; can also specify estimation random effects stimuli simply adding another term formula. example,y ~ x + (1 + x | subject_id) + (1 + x | stimulus_id)regresses y x -subject random intercepts slopes -stimulus random intercepts slopes. way, fitted model capture two sources uncertainty estimates---uncertainty introduced sampling subjects well uncertainty introduced sampling items. Now estimating two independent covariance matrices, one subjects one items. example, matrices 2x2 structure, need case. can flexibly change random effects structure changing formula specification left side bar | symbol. instance, another predictor w, might :y ~ x + (x | subject_id) + (x + w | stimulus_id)estimate 2x2 matrix subjects, covariance matrix stimuli now 3x3 matrix (intercepts, slope x, slope w). Although enables great flexibility, random effects structure becomes complex, estimation process becomes difficult less likely converge upon result.","code":""},{"path":"linear-mixed-effects-models-with-crossed-random-factors.html","id":"specifying-random-effects","chapter":"7 Linear mixed-effects models with crossed random factors","heading":"7.3 Specifying random effects","text":"choice random effects structure new problem appeared linear mixed-effects models. traditional approaches using t-test ANOVA, choose random effects structure implicitly choose procedure use. discussed last chapter, choose paired samples t-test independent samples t-test, analogous choosing fit lme4::lmer(y ~ x + (1 | subject)) lm(y ~ x). Likewise, can run mixed model ANOVA either aov(y ~ x + Error(subject_id)), equivalent random intercepts model, aov(y ~ x + Error(x / subject_id)), equivalent random slopes model. tradition psychology performing confirmatory analyses use maximal random effects structure justified design study. , one-factor data pseudoreplications, use aov(y ~ x + Error(x / subject_id)) rather aov(y ~ x + Error(subject_id)). Analogously, analyze data pseudoreplications using linear mixed effects model, use lme4::lmer(y ~ x + (1 + x | subject_id)) rather lme4::lmer(y ~ x + (1 | subject_id)). words, account sources non-independence introduced repeated sampling subjects stimuli. approach known maximal random effects approach design-driven approach specifying random effects structure (Barr et al., 2013). Failing account dependencies introduced design likely lead standard errors small, turn, lead p-values smaller , thus, higher false positive (Type error) rates. cases, can lead lower power, thus higher false negative (Type II error) rate. thus critical importance pay close attention random effects structure performing analyses.Linear mixed-effects models almost inevitably include random intercepts random factor included design. random factors subjects stimuli identified subject_id stimulus_id respectively, least, model syntax include (1 | subject_id) + (1 | stimulus_id). various predictors model, key question becomes: predictors allow vary sampling units? instance, fixed-effects part model 2x2 factorial design factors B, y ~ * b + ..., large variety random effects structures, including (limited ):random intercepts : y ~ * b + (1 | subject_id) + (1 | stimulus_id)-subjects random intercepts -stimulus random intercepts: y ~ * b + (| subject_id) + (1 | stimulus_id)-subjects random intercepts slopes b ab interaction, -stimulus random intercepts: y ~ * b + (* b | subject_id) + (1 | stimulus_id)-subjects random intercepts slopes b ab interaction, -stimulus random intercepts slopes b ab interaction, y ~ * b + (* b | subject_id) + (* b | stimulus_id).important clear one thing.\"maximal random effects structure justified design\" \"maximum possible random effects structure\"; , entail automatically putting random slopes random factors predictors model. follow guidelines random effects next section decide whether inclusion particular random slope , fact, \"justified design.\"authors suggest \"data-driven\" alternative design-driven random effects, suggesting researchers include random slopes justified design justified data (Matuschek et al., 2017). example, might use null-hypothesis test determine whether including -subject random slope x significantly improves model fit, include effect . Although potentially improve power test theoretical interest random slopes small, also exposes additional unknown risk false positives, questionable whether right approach confirmatory context. Thus, recommend data-driven approach.","code":""},{"path":"linear-mixed-effects-models-with-crossed-random-factors.html","id":"rules-for-choosing-random-effects-for-categorical-factors","chapter":"7 Linear mixed-effects models with crossed random factors","heading":"7.3.1 Rules for choosing random effects for categorical factors","text":"random effects structure linear mixed-effects model---words, assumptions effects vary sampling units---absolutely critical ensuring parameters reflect uncertainty introduced sampling (Barr et al., 2013). First , note focused predictors representing design variables theoretical interest perform inferential tests. predictors represent control variables, intend perform statistical tests, unlikely random slopes needed.following rules derived Barr et al. (2013) Barr (2013). Consult papers wish find guidelines. Keep mind can use mixed effects model repeated measures data, either pseudoreplications /presence within-subject (within-stimulus) factors. crossed random factors, inevitably pseudoreplications---multiple observations per subject due multiple stimuli, multiple observations per stimulus due multiple subjects. key determining random effects structure figuring factors within-subjects within-stimuli, pseudoreplications located design. apply rules subjects determine form (1 + ... | subject_id) part formula, stimuli determine form (1 + ... | stimulus_id) part formula. see word \"unit\" \"sampling unit\" , substitute \"subject\" \"stimuli\" needed.rules:repeated measures sampling units, need random intercept random factor: (1 | unit_id);factor x -unit, need random slope factor;Determine highest order interaction within-subject factors unit consideration. pseudoreplications within cell defined combinations (.e., multiple observations per cell), unit need slope interaction well lower order effects. pseudoreplications, need random slopes.first two rules straightforward, third requires explanation. first ask: know whether factor within unit?simple way determine whether factor within use dplyr::count() function, gives frequency counts, loaded load tidyverse. say interested whether factor \\(\\) within subjects, imaginary 2x2x2 factorial data abc_data \\(\\), \\(B\\), \\(C\\) name factors design.see whether \\(\\) within subjects, use:resulting table, can see subject gets levels \\(\\), making within-subject factor. \\(B\\) \\(C\\)?OK \\(B\\) subjects (subject gets one level), \\(C\\) within (subject gets levels).ExerciseAnswer question abc_data.levels factor \\(\\) administered within stimuli? betweenwithinAre levels factor \\(B\\) administered within stimuli? betweenwithinAre levels factor \\(C\\) administered within stimuli? betweenwithinOK, identified factors within subject, factors within stimulus.second rule tells us factor -unit, need random slope factor. Indeed, possible estimate random slope unit factor. think fact random slopes capture variation effect units, makes sense measure response variable across levels factor able estimate variation. instance, two-level factor called treatment group (experimental, control), estimate effect \"treatment\" particular subject unless subject experienced levels factor (.e., within-subject).now apply third rule determine random slopes needed within-unit factors?Consider \\(\\) \\(C\\) within-subjects, \\(B\\) . highest-order interaction within-subject factors \\(AC\\). need random slopes \\(AC\\) interaction well main effects \\(\\) \\(C\\) pseudoreplications subject combination \\(AC\\). can find ?shows us one observation per combination \\(AC\\), need random slopes \\(AC\\), \\(\\) \\(C\\). random effects part formula subjects just (1 | subject_id).random slopes need random factor stimulus?one within-stimulus factor, \\(B\\), pseudoreplications.Therefore formula need stimuli (B | stimulus_id), making full lme4 formula:y ~ * B * C + (1 | subject_id) + (B | stimulus_id).","code":"\nlibrary(\"tidyverse\")\n\n## run this code to create the table \"abc_data\"\nabc_subj <- tibble(subject_id = 1:4,\n                   B = rep(c(\"B1\", \"B2\"), times = 2))\n\nabc_item  <- tibble(stimulus_id = 1:4,\n                    A = rep(c(\"A1\", \"A2\"), each = 2),\n                    C = rep(c(\"C1\", \"C2\"), times = 2))\n\nabc_data <- crossing(abc_subj, abc_item) %>%\n  select(subject_id, stimulus_id, everything())\nabc_data %>%\n  count(subject_id, A)## # A tibble: 8 × 3\n##   subject_id A         n\n##        <int> <chr> <int>\n## 1          1 A1        2\n## 2          1 A2        2\n## 3          2 A1        2\n## 4          2 A2        2\n## 5          3 A1        2\n## 6          3 A2        2\n## 7          4 A1        2\n## 8          4 A2        2\nabc_data %>%\n  count(subject_id, B)## # A tibble: 4 × 3\n##   subject_id B         n\n##        <int> <chr> <int>\n## 1          1 B1        4\n## 2          2 B2        4\n## 3          3 B1        4\n## 4          4 B2        4\nabc_data %>%\n  count(subject_id, C)## # A tibble: 8 × 3\n##   subject_id C         n\n##        <int> <chr> <int>\n## 1          1 C1        2\n## 2          1 C2        2\n## 3          2 C1        2\n## 4          2 C2        2\n## 5          3 C1        2\n## 6          3 C2        2\n## 7          4 C1        2\n## 8          4 C2        2\nabc_data %>%\n  count(stimulus_id, A)## # A tibble: 4 × 3\n##   stimulus_id A         n\n##         <int> <chr> <int>\n## 1           1 A1        4\n## 2           2 A1        4\n## 3           3 A2        4\n## 4           4 A2        4\nabc_data %>%\n  count(stimulus_id, B)## # A tibble: 8 × 3\n##   stimulus_id B         n\n##         <int> <chr> <int>\n## 1           1 B1        2\n## 2           1 B2        2\n## 3           2 B1        2\n## 4           2 B2        2\n## 5           3 B1        2\n## 6           3 B2        2\n## 7           4 B1        2\n## 8           4 B2        2\nabc_data %>%\n  count(stimulus_id, C)## # A tibble: 4 × 3\n##   stimulus_id C         n\n##         <int> <chr> <int>\n## 1           1 C1        4\n## 2           2 C2        4\n## 3           3 C1        4\n## 4           4 C2        4\nabc_data %>%\n  count(subject_id, A, C)## # A tibble: 16 × 4\n##    subject_id A     C         n\n##         <int> <chr> <chr> <int>\n##  1          1 A1    C1        1\n##  2          1 A1    C2        1\n##  3          1 A2    C1        1\n##  4          1 A2    C2        1\n##  5          2 A1    C1        1\n##  6          2 A1    C2        1\n##  7          2 A2    C1        1\n##  8          2 A2    C2        1\n##  9          3 A1    C1        1\n## 10          3 A1    C2        1\n## 11          3 A2    C1        1\n## 12          3 A2    C2        1\n## 13          4 A1    C1        1\n## 14          4 A1    C2        1\n## 15          4 A2    C1        1\n## 16          4 A2    C2        1\nabc_data %>%\n  count(stimulus_id, B)## # A tibble: 8 × 3\n##   stimulus_id B         n\n##         <int> <chr> <int>\n## 1           1 B1        2\n## 2           1 B2        2\n## 3           2 B1        2\n## 4           2 B2        2\n## 5           3 B1        2\n## 6           3 B2        2\n## 7           4 B1        2\n## 8           4 B2        2"},{"path":"linear-mixed-effects-models-with-crossed-random-factors.html","id":"troubleshooting-non-convergence-and-singular-fits","chapter":"7 Linear mixed-effects models with crossed random factors","heading":"7.3.2 Troubleshooting non-convergence and 'singular fits'","text":"attempt fit models maximal random effects, can run couple different problems. Recall estimation algorithm linear mixed-effects model iterative⸻, step--step manner, fitting algorithm searches parameter values make data likely. Sometimes looks looks find , give , case get 'convergence warning.' happens, probably good idea trust estimates non-converged model, need simplify model structure trying .Another thing can happen get message 'singular fit'. latter message appear estimation procedure yields variance-covariance matrix one random factors either (1) perfect near-perfect (1.00, -1.00) positive negative correlations, (2) one variances close zero, (3) . possibly ok ignore message, also reasonable simplify model structure message goes away.simplify model deal convergence problems singular fit messages? done care. suggest following strategy:Constrain covariance parameters zero. accomplished using double-bar || syntax, e.g., changing (* b | subject) (* b || subject). still run estimation problems, :Inspect parameter estimates non-converging singular model. slope variables zero near zero? Remove re-fit model, repeating step convergence warnings / singular fit messages go away.technical details convergence problems , see ?lme4::convergence ?lme4::isSingular.","code":""},{"path":"linear-mixed-effects-models-with-crossed-random-factors.html","id":"simulating-data-with-crossed-random-factors","chapter":"7 Linear mixed-effects models with crossed random factors","heading":"7.4 Simulating data with crossed random factors","text":"exercises, generate simulated data corresponding experiment single, two-level factor (independent variable) within-subjects -items. imagine experiment involves lexical decisions set words (e.g., \"PINT\" word nonword?), dependent variable response time (milliseconds), independent variable word type (noun vs verb). want treat subjects words random factors (can generalize population events subjects encounter words). can play around web app (click open new window), allows manipulate data-generating parameters see effect data.now, pieces puzzle need simulate data study crossed random effects. DeBruine & Barr (2020) provides detailed, step--step walkthrough exercise .DGP response time \\(Y_{si}\\) subject \\(s\\) item \\(\\):Level 1:\\[\\begin{equation}\nY_{si} = \\beta_{0s} + \\beta_{1} X_{} + e_{si}\n\\end{equation}\\]Level 2:\\[\\begin{equation}\n\\beta_{0s} = \\gamma_{00} + S_{0s} + I_{0i}\n\\end{equation}\\]\\[\\begin{equation}\n\\beta_{1} = \\gamma_{10} + S_{1s}\n\\end{equation}\\]Variance Components:\\[\\begin{equation}\n\\langle S_{0s}, S_{1s} \\rangle \\sim N\\left(\\langle 0, 0 \\rangle, \\mathbf{\\Sigma}\\right) \n\\end{equation}\\]\\[\\begin{equation}\n\\mathbf{\\Sigma} = \\left(\\begin{array}{cc}{\\tau_{00}}^2 & \\rho\\tau_{00}\\tau_{11} \\\\\n         \\rho\\tau_{00}\\tau_{11} & {\\tau_{11}}^2 \\\\\n         \\end{array}\\right) \n\\end{equation}\\]\\[\\begin{equation}\nI_{0s} \\sim N\\left(0, {\\omega_{00}}^2\\right) \n\\end{equation}\\]\\[\\begin{equation}\ne_{si} \\sim N\\left(0, \\sigma^2\\right)\n\\end{equation}\\]equation, \\(X_i\\) numerical predictor coding condition item \\(\\) ; e.g., -.5 noun, .5 verb.just reduce levels 1 2 \\[Y_{si} = \\beta_0 + S_{0s} + I_{0i} + (\\beta_1 + S_{1s})X_{} + e_{si}\\]:","code":""},{"path":"linear-mixed-effects-models-with-crossed-random-factors.html","id":"set-up-the-environment-and-define-the-parameters-for-the-dgp","chapter":"7 Linear mixed-effects models with crossed random factors","heading":"7.4.1 Set up the environment and define the parameters for the DGP","text":"want get results everyone else exercise, seed random number generator value. , load packages need.Now define parameters DGP (data generating process).create three tables:merge together information three tables, calculate response variable according model formula .","code":"\nlibrary(\"lme4\")\nlibrary(\"tidyverse\")\n\nset.seed(11709)  \nnsubj <- 100 # number of subjects\nnitem <- 50  # must be an even number\n\nb0 <- 800 # grand mean\nb1 <- 80 # 80 ms difference\neffc <- c(-.5, .5) # deviation codes\n\nomega_00 <- 80 # by-item random intercept sd (omega_00)\n\n## for the by-subjects variance-covariance matrix\ntau_00 <- 100 # by-subject random intercept sd\ntau_11 <- 40 # by-subject random slope sd\nrho <- .2 # correlation between intercept and slope\n\nsig <- 200 # residual (standard deviation)"},{"path":"linear-mixed-effects-models-with-crossed-random-factors.html","id":"generate-a-sample-of-stimuli","chapter":"7 Linear mixed-effects models with crossed random factors","heading":"7.4.2 Generate a sample of stimuli","text":"randomly generate 50 items. Create tibble called item like one , iri -item random intercepts (drawn normal distribution variance \\(\\omega_{00}^2\\) = 6400). Half words type NOUN (cond = -.5) half type VERB (cond = .5).rep()rnorm()","code":"## # A tibble: 50 × 3\n##    item_id  cond    I_0i\n##      <int> <dbl>   <dbl>\n##  1       1  -0.5   14.9 \n##  2       2   0.5  -86.3 \n##  3       3  -0.5  -12.8 \n##  4       4   0.5  -13.9 \n##  5       5  -0.5   55.6 \n##  6       6   0.5  -45.9 \n##  7       7  -0.5  -42.0 \n##  8       8   0.5  -87.6 \n##  9       9  -0.5  -97.4 \n## 10      10   0.5  -85.2 \n## 11      11  -0.5  135.  \n## 12      12   0.5   83.2 \n## 13      13  -0.5  -44.7 \n## 14      14   0.5    8.59\n## 15      15  -0.5 -156.  \n## 16      16   0.5  -57.6 \n## 17      17  -0.5  -38.7 \n## 18      18   0.5   39.6 \n## 19      19  -0.5  105.  \n## 20      20   0.5   30.3 \n## 21      21  -0.5 -115.  \n## 22      22   0.5   -3.40\n## 23      23  -0.5 -218.  \n## 24      24   0.5   53.0 \n## 25      25  -0.5  -86.9 \n## 26      26   0.5  -65.4 \n## 27      27  -0.5  172.  \n## 28      28   0.5 -152.  \n## 29      29  -0.5   25.1 \n## 30      30   0.5 -156.  \n## 31      31  -0.5   47.7 \n## 32      32   0.5  -46.3 \n## 33      33  -0.5   48.0 \n## 34      34   0.5   62.8 \n## 35      35  -0.5  -75.4 \n## 36      36   0.5  -35.9 \n## 37      37  -0.5  -48.5 \n## 38      38   0.5   29.3 \n## 39      39  -0.5  -55.5 \n## 40      40   0.5   69.5 \n## 41      41  -0.5  196.  \n## 42      42   0.5   77.6 \n## 43      43  -0.5  -45.0 \n## 44      44   0.5  204.  \n## 45      45  -0.5   32.1 \n## 46      46   0.5  -63.9 \n## 47      47  -0.5  145.  \n## 48      48   0.5   66.2 \n## 49      49  -0.5  -23.9 \n## 50      50   0.5   97.3\nitems <- tibble(item_id = 1:nitem,\n                cond = rep(c(-.5, .5), times = nitem / 2),\n                I_0i = rnorm(nitem, 0, sd = omega_00))"},{"path":"linear-mixed-effects-models-with-crossed-random-factors.html","id":"generate-a-sample-of-subjects","chapter":"7 Linear mixed-effects models with crossed random factors","heading":"7.4.3 Generate a sample of subjects","text":"generate -subject random effects, need generate data bivariate normal distribution. , use function MASS::mvrnorm.REMEMBER: run library(\"MASS\") just get one function, MASS function select() overwrite tidyverse version. Since want MASS mvrnorm() function, can just access directly pkgname::function syntax, .e., MASS::mvrnorm().subjects table look like :recall :tau_00: -subject random intercept standard deviationtau_11: -subject random slope standard deviationrho : correlation intercept slopecovariance = rho * tau_00 * tau_11","code":"## # A tibble: 100 × 3\n##     subj_id      S_0s     S_1s\n##       <int>     <dbl>    <dbl>\n##   1       1  -80.0      -0.763\n##   2       2   44.6      54.5  \n##   3       3    8.74    -20.4  \n##   4       4  -38.6     -23.8  \n##   5       5  -83.3      29.2  \n##   6       6  -70.9     -13.8  \n##   7       7  -21.4      46.0  \n##   8       8    2.33      8.39 \n##   9       9   62.3     -58.2  \n##  10      10  238.        7.72 \n##  11      11  -92.5       2.14 \n##  12      12   58.5     -65.8  \n##  13      13 -204.      -38.8  \n##  14      14  -91.6       5.46 \n##  15      15   51.1     -38.8  \n##  16      16  142.      -12.9  \n##  17      17   46.0       6.60 \n##  18      18  -56.7     -54.8  \n##  19      19  -10.1      62.1  \n##  20      20 -226.      -19.3  \n##  21      21 -158.      -18.5  \n##  22      22  102.        8.99 \n##  23      23  -12.7     -70.6  \n##  24      24  135.       -9.50 \n##  25      25   62.0     -52.5  \n##  26      26    0.0653   32.8  \n##  27      27 -117.       70.8  \n##  28      28 -232.        3.43 \n##  29      29   70.9      50.8  \n##  30      30 -123.       22.8  \n##  31      31  268.       30.0  \n##  32      32  -18.7     -25.0  \n##  33      33   50.8     -31.0  \n##  34      34  -43.1     -28.9  \n##  35      35  -10.1      28.3  \n##  36      36   65.6      18.2  \n##  37      37 -123.       -4.63 \n##  38      38  -94.8      10.3  \n##  39      39   77.7     -22.5  \n##  40      40  -59.1      52.4  \n##  41      41  -91.2    -103.   \n##  42      42  -66.6      -2.14 \n##  43      43   -4.40      0.305\n##  44      44   69.7      10.2  \n##  45      45  -77.5     -10.4  \n##  46      46  -17.8     -48.2  \n##  47      47 -103.       47.0  \n##  48      48   22.8     -39.3  \n##  49      49  -31.1     -34.9  \n##  50      50  -26.4      40.0  \n##  51      51   47.8      26.0  \n##  52      52  -93.2     -42.7  \n##  53      53   28.9      51.4  \n##  54      54  -19.3      11.5  \n##  55      55   53.6      21.5  \n##  56      56  -27.4     -21.4  \n##  57      57  -67.7     -32.1  \n##  58      58   59.2      13.4  \n##  59      59  -53.1       2.44 \n##  60      60  104.        7.41 \n##  61      61  -20.7     -78.7  \n##  62      62   55.9     -15.7  \n##  63      63  114.      -29.1  \n##  64      64  -57.7     -34.7  \n##  65      65  -38.7      -9.14 \n##  66      66 -106.      -58.0  \n##  67      67   99.1     -37.6  \n##  68      68  -56.9      21.0  \n##  69      69  -50.4      -0.407\n##  70      70   27.5      -2.69 \n##  71      71  139.      -32.2  \n##  72      72   44.9       8.53 \n##  73      73  -14.8      71.7  \n##  74      74   33.7     -52.6  \n##  75      75    2.03     27.8  \n##  76      76 -134.       37.0  \n##  77      77   24.4      20.7  \n##  78      78  -60.6     -36.7  \n##  79      79   31.1      16.9  \n##  80      80  -34.9       9.68 \n##  81      81  206.       17.3  \n##  82      82   -7.19    -25.4  \n##  83      83  182.       46.0  \n##  84      84   55.7      21.7  \n##  85      85 -149.      -44.0  \n##  86      86 -193.      -73.2  \n##  87      87  167.       13.9  \n##  88      88  160.        3.87 \n##  89      89   84.1      82.1  \n##  90      90   97.2      -6.55 \n##  91      91 -205.     -125.   \n##  92      92  -75.1       6.76 \n##  93      93  -95.3     -46.5  \n##  94      94  106.       38.6  \n##  95      95  -42.4      11.3  \n##  96      96   74.0     -21.1  \n##  97      97 -245.      -25.3  \n##  98      98 -113.       -1.88 \n##  99      99   68.8      30.6  \n## 100     100  136.       44.2\nmatrix(    tau_00^2,            rho * tau_00 * tau_11,\n        rho * tau_00 * tau_11,      tau_11^2, ...)\nas_tibble(mx) %>%\n  mutate(subj_id = ...)\ncov <- rho * tau_00 * tau_11\n\nmx <- matrix(c(tau_00^2, cov,\n               cov,      tau_11^2),\n             nrow = 2)\n\nby_subj_rfx <- MASS::mvrnorm(nsubj, mu = c(S_0s = 0, S_1s = 0), Sigma = mx)\n\nsubjects <- as_tibble(by_subj_rfx) %>%\n  mutate(subj_id = row_number()) %>%\n  select(subj_id, everything())"},{"path":"linear-mixed-effects-models-with-crossed-random-factors.html","id":"generate-a-sample-of-encounters-trials","chapter":"7 Linear mixed-effects models with crossed random factors","heading":"7.4.4 Generate a sample of encounters (trials)","text":"trial encounter particular subject stimulus. experiment, subject see stimulus. Generate table trials lists encounters experiments. Note: participant encounters stimulus item . Use crossing() function create possible encounters.Now apply example generate table , err residual term, drawn \\(N \\sim \\left(0, \\sigma^2\\right)\\), \\(\\sigma\\) err_sd.","code":"## # A tibble: 5,000 × 3\n##    subj_id item_id    err\n##      <int>   <int>  <dbl>\n##  1       1       1  382. \n##  2       1       2  283. \n##  3       1       3   30.4\n##  4       1       4 -282. \n##  5       1       5 -239. \n##  6       1       6   73.4\n##  7       1       7  -98.4\n##  8       1       8 -189. \n##  9       1       9 -410. \n## 10       1      10  102. \n## # ℹ 4,990 more rows\ntrials <- crossing(subj_id = subjects %>% pull(subj_id),\n                   item_id = items %>% pull(item_id)) %>%\n  mutate(err = rnorm(nrow(.), mean = 0, sd = sig))"},{"path":"linear-mixed-effects-models-with-crossed-random-factors.html","id":"join-subjects-items-and-trials","chapter":"7 Linear mixed-effects models with crossed random factors","heading":"7.4.5 Join subjects, items, and trials","text":"Merge information subjects, items, trials create full dataset dat_sim, looks like :inner_join()","code":"## # A tibble: 5,000 × 7\n##    subj_id item_id  S_0s  I_0i   S_1s  cond    err\n##      <int>   <int> <dbl> <dbl>  <dbl> <dbl>  <dbl>\n##  1       1       1 -80.0  14.9 -0.763  -0.5  382. \n##  2       1       2 -80.0 -86.3 -0.763   0.5  283. \n##  3       1       3 -80.0 -12.8 -0.763  -0.5   30.4\n##  4       1       4 -80.0 -13.9 -0.763   0.5 -282. \n##  5       1       5 -80.0  55.6 -0.763  -0.5 -239. \n##  6       1       6 -80.0 -45.9 -0.763   0.5   73.4\n##  7       1       7 -80.0 -42.0 -0.763  -0.5  -98.4\n##  8       1       8 -80.0 -87.6 -0.763   0.5 -189. \n##  9       1       9 -80.0 -97.4 -0.763  -0.5 -410. \n## 10       1      10 -80.0 -85.2 -0.763   0.5  102. \n## # ℹ 4,990 more rows\ndat_sim <- subjects %>%\n  inner_join(trials, \"subj_id\") %>%\n  inner_join(items, \"item_id\") %>%\n  arrange(subj_id, item_id) %>%\n  select(subj_id, item_id, S_0s, I_0i, S_1s, cond, err)"},{"path":"linear-mixed-effects-models-with-crossed-random-factors.html","id":"create-the-response-variable","chapter":"7 Linear mixed-effects models with crossed random factors","heading":"7.4.6 Create the response variable","text":"Add response variable Y dat according model formula:\\[Y_{si} = \\beta_0 + S_{0s} + I_{0i} + (\\beta_1 + S_{1s})X_{} + e_{si}\\]resulting table (dat_sim2) looks like :Note: full decomposition table model.","code":"## # A tibble: 5,000 × 8\n##    subj_id item_id     Y  S_0s  I_0i   S_1s  cond    err\n##      <int>   <int> <dbl> <dbl> <dbl>  <dbl> <dbl>  <dbl>\n##  1       1       1 1078. -80.0  14.9 -0.763  -0.5  382. \n##  2       1       2  957. -80.0 -86.3 -0.763   0.5  283. \n##  3       1       3  698. -80.0 -12.8 -0.763  -0.5   30.4\n##  4       1       4  464. -80.0 -13.9 -0.763   0.5 -282. \n##  5       1       5  497. -80.0  55.6 -0.763  -0.5 -239. \n##  6       1       6  787. -80.0 -45.9 -0.763   0.5   73.4\n##  7       1       7  540. -80.0 -42.0 -0.763  -0.5  -98.4\n##  8       1       8  483. -80.0 -87.6 -0.763   0.5 -189. \n##  9       1       9  173. -80.0 -97.4 -0.763  -0.5 -410. \n## 10       1      10  776. -80.0 -85.2 -0.763   0.5  102. \n## # ℹ 4,990 more rows... %>% \n  mutate(Y = ...) %>%\n  select(...)\ndat_sim2 <- dat_sim %>%\n  mutate(Y = b0 + S_0s + I_0i + (S_1s + b1) * cond + err) %>%\n  select(subj_id, item_id, Y, everything())"},{"path":"linear-mixed-effects-models-with-crossed-random-factors.html","id":"fitting-the-model","chapter":"7 Linear mixed-effects models with crossed random factors","heading":"7.4.7 Fitting the model","text":"Now created simulated data, estimate model using lme4::lmer(), run summary().Now see can identify data generating parameters output summary().First, try find \\(\\beta_0\\) \\(\\beta_1\\).Now try find estimates random effects parameters \\(\\tau_{00}\\), \\(\\tau_{11}\\), \\(\\rho\\), \\(\\omega_{00}\\), \\(\\sigma\\).","code":"\nmod_sim <- lmer(Y ~ cond + (1 + cond | subj_id) + (1 | item_id),\n                dat_sim2, REML = FALSE)\n\nsummary(mod_sim, corr = FALSE)## Linear mixed model fit by maximum likelihood  ['lmerMod']\n## Formula: Y ~ cond + (1 + cond | subj_id) + (1 | item_id)\n##    Data: dat_sim2\n## \n##      AIC      BIC   logLik deviance df.resid \n##  67639.4  67685.0 -33812.7  67625.4     4993 \n## \n## Scaled residuals: \n##     Min      1Q  Median      3Q     Max \n## -3.6357 -0.6599 -0.0251  0.6767  3.7685 \n## \n## Random effects:\n##  Groups   Name        Variance Std.Dev. Corr\n##  subj_id  (Intercept)  9464.8   97.29       \n##           cond          597.7   24.45   0.68\n##  item_id  (Intercept)  8087.0   89.93       \n##  Residual             40305.0  200.76       \n## Number of obs: 5000, groups:  subj_id, 100; item_id, 50\n## \n## Fixed effects:\n##             Estimate Std. Error t value\n## (Intercept)   793.29      16.26  48.782\n## cond           77.65      26.18   2.967"},{"path":"generalized-linear-mixed-effects-models.html","id":"generalized-linear-mixed-effects-models","chapter":"8 Generalized linear mixed-effects models","heading":"8 Generalized linear mixed-effects models","text":"","code":""},{"path":"generalized-linear-mixed-effects-models.html","id":"discrete-versus-continuous-data","chapter":"8 Generalized linear mixed-effects models","heading":"8.1 Discrete versus continuous data","text":"models considering point assumed response (.e. dependent) variable measuring continuous numeric. However, many cases psychology measurements discrete nature. One type discrete data involves finite set possible values gaps values, get Likert scale data. Another type discrete data response variable reflects categories intrinsic ordering (often called \"nominal\" data), whether customer restaurant orders meal chicken, beef, tofu.Discrete data common psychology. examples discrete data:type linguistic structure speaker produces (double object prepositional object phrase);set images participant viewing given moment;whether participant made accurate inaccurate selection;whether job candidate gets hired ;agreement Likert scale.Another common type data count data, values also discrete. Often count data, number opportunities something occur well-defined. examples:number speech errors corpus natural language;number car accidents occuring year given intersection;number visits doctor given month.","code":""},{"path":"generalized-linear-mixed-effects-models.html","id":"why-not-model-discrete-data-as-continuous","chapter":"8 Generalized linear mixed-effects models","heading":"8.1.1 Why not model discrete data as continuous?","text":"Discrete data properties generally make bad idea try analyze using models intended continuous data. instance, interested probability binary event (participant's accuracy forced-choice task), measurement represented 0 1, indicating inaccurate accurate response, respectively. calculate proportion accurate responses participant analyze (many people ) bad idea number reasons.","code":""},{"path":"generalized-linear-mixed-effects-models.html","id":"bounded-scale","chapter":"8 Generalized linear mixed-effects models","heading":"8.1.1.1 Bounded scale","text":"Discrete data generally bounded scale. may bounded (count data, lower bound zero) may upper lower bound, Likert scale data binary data. attempt model bounded data approach intended continuous data, model may end assigning non-zero probabilities impossible values outside scale.Analyzing bounded data models continuous data can lead detection spurious interaction effects. instance, consider effect experimental intervention increases accuracy. participants already highly accurate (e.g., 90%) condition condition B (say, 50%) size possible effect smaller size possible effect B, since accuracy exceed 100%. Thus, difficult know whether interaction effect reflects something theoretically meaningful, just artifact bounded nature scale.","code":""},{"path":"generalized-linear-mixed-effects-models.html","id":"the-variance-depends-on-the-the-mean","chapter":"8 Generalized linear mixed-effects models","heading":"8.1.1.2 The variance depends on the the mean","text":"settings continuous data, variance assumed independent mean; essentially assumption homogeneity variance model continuous predictor. discrete data, assumption independence mean variance often met.can see data simulation. rbinom() function makes possible simulate data binomial distribution, describes collection discrete observations behaves. consider, instance, probability rain given day Barcelona, Spain, versus Glasgow, U.K. According website, Barcelona gets average 55 days rain per year, Glasgow gets 170. probability rain given day Glasgow can estimated 170/365 0.47, probability Barcelona 55/365 0.15. simulate 500 years rainfall two cities (assuming constant climate).can see greater variability Glasgow look standard deviations simulated data city.binomially distributed data, variance given \\(np(1-p)\\) \\(n\\) number observations \\(p\\) probability 'success' (example, probability rain given day). plot shows \\(n=1000\\); note variance peaks 0.5 gets small probability approaches 0 1.\nFigure 8.1: Plot variance versus probability, sample size \\(n = 1000\\).\n","code":"\nrainy_days <- tibble(city = rep(c(\"Barcelona\", \"Glasgow\"), each = 500),\n       days_of_rain = c(rbinom(500, 365, 55/365),\n                        rbinom(500, 365, 170/365))) \nrainy_days %>%\n  group_by(city) %>%\n  summarise(variance = var(days_of_rain))## # A tibble: 2 × 2\n##   city      variance\n##   <chr>        <dbl>\n## 1 Barcelona     50.4\n## 2 Glasgow       89.9"},{"path":"generalized-linear-mixed-effects-models.html","id":"generalized-linear-models","chapter":"8 Generalized linear mixed-effects models","heading":"8.2 Generalized Linear Models","text":"basic idea behind Generalized Linear Models (confused General Linear Models) specify link function transforms response space modeling space can perform usual linear regression, capture dependence variance mean variance function. parameters model expressed scale modeling space, can always transform back original response space using inverse link function.large variety different kinds generalized linear models can fit different types data. ones commonly used psychology logistic regression Poisson regression, former used binary data (Bernoulli trials) latter used count data, number trials well-defined. focusing logistic regression.","code":""},{"path":"generalized-linear-mixed-effects-models.html","id":"logistic-regression","chapter":"8 Generalized linear mixed-effects models","heading":"8.3 Logistic regression","text":"","code":""},{"path":"generalized-linear-mixed-effects-models.html","id":"terminology","chapter":"8 Generalized linear mixed-effects models","heading":"8.3.1 Terminology","text":"logistic regression, modeling relationship response set predictors log odds space.Logistic regression used individual outcomes Bernoulli trials⸻events binary outcomes. Typically one two outcomes referred 'success' coded 1; referred 'failure' coded 0. Note terms 'success' 'failure' completely arbitrary, taken imply desireable category always coded 1. instance, flipping coin equivalently choose 'heads' success 'tails' failure vice-versa.Often outcome sequence Bernoulli trials communicated proportion⸻ratio successes total number trials. instance, flip coin 100 times get 47 heads, proportion 47/100 .47, also estimate probability event. events coded 1s 0s, shortcut way getting proportion use mean() function.can also talk odds success, .e., odds heads versus tails one one, 1:1. odds raining given day Glasgow 170:195; denominator number days rain (365 - 170 = 195). Expressed decimal number, ratio 170/195 0.87, known natural odds. Natural odds ranges 0 \\(+\\inf\\). Given \\(Y\\) successes \\(N\\) trials, can represent natural odds \\(\\frac{Y}{N - Y}\\). , given probability \\(p\\), can represent odds \\(\\frac{p}{1-p}\\).natural log odds, logit scale logistic regression performed. Recall logarithm value \\(Y\\) gives exponent yield \\(Y\\) given base. instance, \\(log_2\\) (log base 2) 16 4, \\(2^4 = 16\\). logistic regression, base typically used \\(e\\) (also known Euler's number). get log odds odds , say, Glasgow rainfall, use log(170/195), yields -0.1372011; get natural odds back log odds, use inverse, exp(-.137), returns 0.872.","code":""},{"path":"generalized-linear-mixed-effects-models.html","id":"properties-of-log-odds","chapter":"8 Generalized linear mixed-effects models","heading":"8.3.2 Properties of log odds","text":"log odds = \\(\\log \\left(\\frac{p}{1-p}\\right)\\)Log odds nice properties linear modeling.First, symmetric around zero, zero log odds corresponds maximum uncertainty, .e., probability .5. Positive log odds means success likely failure (Pr(success) > .5), negative log odds means failure likely success (Pr(success) < .5). log odds 2 means success likely failure amount -2 means failure likely success. scale unbounded; goes \\(-\\infty\\) \\(+\\infty\\).","code":""},{"path":"generalized-linear-mixed-effects-models.html","id":"link-and-variance-functions","chapter":"8 Generalized linear mixed-effects models","heading":"8.3.3 Link and variance functions","text":"link function logistic regression :\\[\\eta = \\log \\left(\\frac{p}{1-p}\\right)\\]inverse link function :\\[p = \\frac{1}{1 + e^{-\\eta}}\\]\\(e\\) Euler's number. R, type latter function 1/(1 + exp(-eta)).variance function variance binomial distribution, namely:\\[np(1 - p).\\]app allows manipulate intercept slope line log odds space see projection line back response space. Note S-shaped (\"sigmoidal\") shape function response shape.\nFigure 8.2: Logistic regression web app https://talklab.psy.gla.ac.uk/app/logit-site/\n","code":""},{"path":"generalized-linear-mixed-effects-models.html","id":"estimating-logistic-regression-models-in-r","chapter":"8 Generalized linear mixed-effects models","heading":"8.3.4 Estimating logistic regression models in R","text":"single-level data, use glm() function. Note much like lm() function already familiar . main difference specify family argument link/variance functions. logistic regression, use family = binomial(link = \"logit\"). logit link default binomial family logit link, typing family = binomial sufficient.glm(DV ~ IV1 + IV2 + ..., data, family = binomial)multi-level data random effects modeled, use glmer function lme4:glmer(DV ~ IV1 + IV2 + ... (1 | subject), data, family = binomial)","code":""},{"path":"modeling-ordinal-data.html","id":"modeling-ordinal-data","chapter":"9 Modeling Ordinal Data","heading":"9 Modeling Ordinal Data","text":"Nothing see (yet)! Stay tuned...","code":""},{"path":"symbols.html","id":"symbols","chapter":"A Symbols","heading":"A Symbols","text":"","code":""},{"path":"symbols.html","id":"general-notes","chapter":"A Symbols","heading":"A.1 General notes","text":"Greek letters represent population parameter values; roman letters represent sample values.Greek letters represent population parameter values; roman letters represent sample values.Greek letter \"hat\" represents estimate population value sample; .e., \\(\\mu_x\\) represents true population mean \\(X\\), \\(\\hat{\\mu}_x\\) represents estimate sample.Greek letter \"hat\" represents estimate population value sample; .e., \\(\\mu_x\\) represents true population mean \\(X\\), \\(\\hat{\\mu}_x\\) represents estimate sample.","code":""},{"path":"symbols.html","id":"table-of-symbols","chapter":"A Symbols","heading":"A.2 Table of symbols","text":"","code":""},{"path":"bibliography.html","id":"bibliography","chapter":"B Bibliography","heading":"B Bibliography","text":"Baayen, R. H., Davidson, D. J., & Bates, D. M. (2008). Mixed-effects modeling crossed random effects subjects items. Journal Memory Language, 59(4), 390–412.Barr, D. J. (2013). Random effects structure testing interactions linear mixed-effects models. Frontiers Psychology, 4, 328.Barr, D. J. (2017). Generalizing encounters: Statistical theoretical considerations. Oxford handbook psycholinguistics. Oxford University Press. https://psyarxiv.com/mcrzu/Barr, D. J., Levy, R., Scheepers, C., & Tily, H. J. (2013). Random effects structure confirmatory hypothesis testing: Keep maximal. Journal Memory Language, 68(3), 255–278.Bates, D., Maechler, M., Bolker, B., & Walker, S. (2015). Fitting linear mixed-effects models using lme4. Journal Statistical Software, 67, 1–48.Belenky, G., Wesensten, N. J., Thorne, D. R., Thomas, M. L., Sing, H. C., Redmond, D. P., Russo, M. B., & Balkin, T. J. (2003). Patterns performance degradation restoration sleep restriction subsequent recovery: sleep dose-response study. Journal Sleep Research, 12(1), 1–12.Clark, H. H. (1973). language--fixed-effect fallacy: critique language statistics psychological research. Journal Verbal Learning Verbal Behavior, 12(4), 335–359.Coleman, E. B. (1964). Generalizing language population. Psychological Reports, 14(1), 219–226.DeBruine, L., & Barr, D. J. (2020). Understanding mixed effects models data simulation. Advances Methods Practice Psychological Science. https://psyarxiv.com/xp5cy/Judd, C. M., Westfall, J., & Kenny, D. . (2012). Treating stimuli random factor social psychology: new comprehensive solution pervasive largely ignored problem. Journal Personality Social Psychology, 103, 54–69.Lakens, D., Scheel, . M., & Isager, P. M. (2018). Equivalence testing psychological research: tutorial. Advances Methods Practices Psychological Science, 1, 259–269. https://journals.sagepub.com/doi/abs/10.1177/2515245918770963Luke, S. G. (2017). Evaluating significance linear mixed-effects models r. Behavior Research Methods, 49, 1494–1502.Matuschek, H., Kliegl, R., Vasishth, S., Baayen, H., & Bates, D. (2017). Balancing Type error power linear mixed models. Journal Memory Language, 94, 305–315.McElreath, R. (2020). Statistical Rethinking: Bayesian course examples R STAN. CRC Press.Vanhove, J. (2021). Collinearity isn’t disease needs curing. Meta-Psychology, 5.Yarkoni, T. (2019). generalizability crisis. https://doi.org/10.31234/osf.io/jqw35","code":""}]
